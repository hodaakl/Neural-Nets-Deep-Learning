{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Most general notebook \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "\n",
    "# Form our test and train data\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import load_model\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# \n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the notebook where I load the data \n",
    "# Project1/train-images-idx3-ubyte.gz\n",
    "f_train_im = gzip.open('C:/everything/Courses/EEL6814 - Neural Networks and Deep Learning/Project1/train-images-idx3-ubyte.gz','r')\n",
    "f_train_lab = gzip.open('C:/everything/Courses/EEL6814 - Neural Networks and Deep Learning/Project1/train-labels-idx1-ubyte.gz','r')\n",
    "f_test_im = gzip.open('C:/everything/Courses/EEL6814 - Neural Networks and Deep Learning/Project1/t10k-images-idx3-ubyte.gz','r')\n",
    "f_test_lab = gzip.open('C:/everything/Courses/EEL6814 - Neural Networks and Deep Learning/Project1/t10k-labels-idx1-ubyte.gz','r')\n",
    "image_size = 28\n",
    "lab_size = 1\n",
    "n_train = 60000\n",
    "n_test = 10000\n",
    "f_train_im.read(16)\n",
    "f_train_lab.read(8)\n",
    "f_test_im.read(16)\n",
    "f_test_lab.read(8)\n",
    "buf_tr_im = f_train_im.read(image_size * image_size * n_train)\n",
    "buf_tr_lab = f_train_lab.read(lab_size * n_train)\n",
    "buf_test_im = f_test_im.read(image_size * image_size * n_test)\n",
    "buf_test_lab = f_test_lab.read(lab_size * n_train)\n",
    "train_im = np.frombuffer(buf_tr_im, dtype=np.uint8).astype(np.float32)\n",
    "train_im = train_im.reshape(n_train, image_size, image_size, 1)\n",
    "test_im = np.frombuffer(buf_test_im, dtype=np.uint8).astype(np.float32)\n",
    "test_im = test_im.reshape(n_test, image_size, image_size, 1)\n",
    "train_labels = np.frombuffer(buf_tr_lab, dtype=np.uint8).astype(np.int64)\n",
    "train_labels = train_labels.reshape(n_train,1)\n",
    "test_labels = np.frombuffer(buf_test_lab, dtype=np.uint8).astype(np.int64)\n",
    "test_labels = test_labels.reshape(n_test,1)\n",
    "####### PROCESS THE DATA \n",
    "# Data Processing\n",
    "#scaling \n",
    "# reshape the data to be able to be scaled \n",
    "Train_Im_Reshaped = train_im.reshape(train_im.shape[0],28*28)\n",
    "Test_Im_Reshaped = test_im.reshape(test_im.shape[0],28*28)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scl = MinMaxScaler()\n",
    "scl.fit(Train_Im_Reshaped)\n",
    "Train_Im_Scaled = scl.transform(Train_Im_Reshaped)\n",
    "XTest = scl.transform(Test_Im_Reshaped)\n",
    "#reshape data back to images \n",
    "# Train_Im_2d_np = Train_Im_Scaled.reshape(Train_Im_Scaled.shape[0], 28,28,1)\n",
    "# Test_Im_2d_np  = Test_Im_Scaled.reshape(Test_Im_Scaled.shape[0],28,28,1)\n",
    "YTrainAll_oneHot = to_categorical(train_labels )\n",
    "YTest_oneHot = to_categorical(test_labels)\n",
    "X_Train,X_Val,Y_Train_oneHot,Y_Val_oneHot = train_test_split(Train_Im_Scaled,YTrainAll_oneHot, test_size=0.1, random_state=42, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_Create(nhidden1,  nhidden_in = 0,  nHiddenMoreLayers = 0,  dropoutrate = 0):\n",
    "    # Stop criteria will be a string ; either 'val_loss' or 'val_accuracy'\n",
    "    nout = 10; #number of output nodes\n",
    "#     SavePath = '1layer_nh' + str(nhidden) + '_LR_'+ str(lr)+ '_SC_' + StopCriteria + '_batch_' + str(batchnum) +'_OPT_' + optimization +'.h5'\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(nhidden1,activation='sigmoid',input_shape=(28*28,)))\n",
    "    if dropoutrate > 0: \n",
    "        network.add(layers.Dropout(dropoutrate))\n",
    "    if nHiddenMoreLayers >0 :\n",
    "        for hiddenLayer in range(nHiddenMoreLayers): \n",
    "            network.add(layers.Dense(nhidden_in, activation = 'sigmoid'))\n",
    "    network.add(layers.Dense(nout, activation = 'softmax'))\n",
    "    return  network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### batchnum, epochs, StopCriteria, optimization,\n",
    "def Fit_network(network, SavePath , epochs = 500, batchnum = 128, StopCriteria = 'val_loss', optimization = 'adam', lr =.001, loss='categorical_crossentropy',verbose = 1):\n",
    "    # Optimizer definition\n",
    "    if optimization == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimization == 'SGD':\n",
    "        opt = keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimization == 'RMSProp':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    elif optimization == 'Adadelta':\n",
    "        opt = keras.optimizers.Adadelta(learning_rate=lr)\n",
    "    elif optimization == 'Adagrad':\n",
    "        opt = keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    elif optimization == 'Adamax':\n",
    "        opt = keras.optimizers.Adamax( learning_rate=lr)\n",
    "# \n",
    "#     SavePath = str() + str(nhidden) + '_LR_'+ str(lr)+ '_SC_' + StopCriteria + '_batch_' + str(batchnum) +'_OPT_' + optimization +'.h5'\n",
    "    network.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "#     network.save_weights('model_init.h5')\n",
    "    callbacks = [EarlyStopping(monitor=StopCriteria, patience=20),\n",
    "             ModelCheckpoint(filepath=SavePath, monitor=StopCriteria, save_best_only=True)]\n",
    "    history = network.fit(X_Train,Y_Train_oneHot,\n",
    "                      epochs=epochs,\n",
    "                      batch_size=batchnum,\n",
    "                      callbacks=callbacks, # Early stopping\n",
    "                      validation_data=(X_Val,Y_Val_oneHot),\n",
    "                         verbose = verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(nlayers, nnodes, optimizer = 'adam', lr = 0.001):\n",
    "    path = 'C:/everything/Courses/EEL6814 - Neural Networks and Deep Learning/Project1/Final/SavedModels/11062020_Model_'+ str(nlayers)+'_layer_'+str(nnodes)+'nodes_'+ optimizer +'_Opt_' + str(lr)+'_LR.h5'\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 0.6148 - acc: 0.7932 - val_loss: 0.4597 - val_acc: 0.8375\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.4342 - acc: 0.8451 - val_loss: 0.4184 - val_acc: 0.8467\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3933 - acc: 0.8591 - val_loss: 0.3811 - val_acc: 0.8610\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3697 - acc: 0.8669 - val_loss: 0.3693 - val_acc: 0.8665\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.3505 - acc: 0.8740 - val_loss: 0.3588 - val_acc: 0.8693\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3351 - acc: 0.8776 - val_loss: 0.3605 - val_acc: 0.8727\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.3198 - acc: 0.8841 - val_loss: 0.3378 - val_acc: 0.8770\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.3083 - acc: 0.8877 - val_loss: 0.3248 - val_acc: 0.8800\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.2958 - acc: 0.8910 - val_loss: 0.3268 - val_acc: 0.8822\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.2877 - acc: 0.8939 - val_loss: 0.3201 - val_acc: 0.8790\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.2780 - acc: 0.8972 - val_loss: 0.3290 - val_acc: 0.8787\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.2666 - acc: 0.9024 - val_loss: 0.3110 - val_acc: 0.8883\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.2591 - acc: 0.9041 - val_loss: 0.3029 - val_acc: 0.8907\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2512 - acc: 0.9071 - val_loss: 0.3176 - val_acc: 0.8868\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2430 - acc: 0.9108 - val_loss: 0.3012 - val_acc: 0.8928\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.2367 - acc: 0.9117 - val_loss: 0.3035 - val_acc: 0.8870\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 0.2291 - acc: 0.9159 - val_loss: 0.3032 - val_acc: 0.8927\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.2224 - acc: 0.9186 - val_loss: 0.3060 - val_acc: 0.8885\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.2165 - acc: 0.9206 - val_loss: 0.3030 - val_acc: 0.8875\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 0.2111 - acc: 0.9222 - val_loss: 0.3015 - val_acc: 0.8923\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.2051 - acc: 0.9249 - val_loss: 0.2978 - val_acc: 0.8925\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.2002 - acc: 0.9263 - val_loss: 0.3028 - val_acc: 0.8920\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 0.1950 - acc: 0.9292 - val_loss: 0.2980 - val_acc: 0.8950\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1886 - acc: 0.9304 - val_loss: 0.2927 - val_acc: 0.8955\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 0.1843 - acc: 0.9331 - val_loss: 0.2884 - val_acc: 0.8943\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1782 - acc: 0.9353 - val_loss: 0.3011 - val_acc: 0.8907\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1744 - acc: 0.9360 - val_loss: 0.2987 - val_acc: 0.8960\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1700 - acc: 0.9379 - val_loss: 0.2945 - val_acc: 0.8965\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1656 - acc: 0.9402 - val_loss: 0.3013 - val_acc: 0.8948\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 1s 28us/sample - loss: 0.1596 - acc: 0.9429 - val_loss: 0.2926 - val_acc: 0.8952\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1565 - acc: 0.9445 - val_loss: 0.2966 - val_acc: 0.8947\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1531 - acc: 0.9446 - val_loss: 0.2983 - val_acc: 0.8957\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1477 - acc: 0.9469 - val_loss: 0.2928 - val_acc: 0.8952\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1453 - acc: 0.9476 - val_loss: 0.3093 - val_acc: 0.8922\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1414 - acc: 0.9493 - val_loss: 0.3072 - val_acc: 0.8950\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1372 - acc: 0.9502 - val_loss: 0.2998 - val_acc: 0.8942\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1331 - acc: 0.9530 - val_loss: 0.3047 - val_acc: 0.8950\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1311 - acc: 0.9530 - val_loss: 0.3040 - val_acc: 0.8970\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 0.1253 - acc: 0.9566 - val_loss: 0.3208 - val_acc: 0.8915\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1205 - acc: 0.9570 - val_loss: 0.3097 - val_acc: 0.8952\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1184 - acc: 0.9581 - val_loss: 0.3079 - val_acc: 0.8972\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.1164 - acc: 0.9588 - val_loss: 0.3119 - val_acc: 0.8928\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1116 - acc: 0.9613 - val_loss: 0.3228 - val_acc: 0.8922\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1103 - acc: 0.9613 - val_loss: 0.3124 - val_acc: 0.8943\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1043 - acc: 0.9639 - val_loss: 0.3235 - val_acc: 0.8923\n",
      "WARNING:tensorflow:From C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.3105 - acc: 0.8927\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.6058 - acc: 0.7952 - val_loss: 0.4840 - val_acc: 0.8217\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.4306 - acc: 0.8462 - val_loss: 0.4082 - val_acc: 0.8518\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.3941 - acc: 0.8583 - val_loss: 0.3798 - val_acc: 0.8608\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.3698 - acc: 0.8670 - val_loss: 0.3724 - val_acc: 0.8663\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.3484 - acc: 0.8740 - val_loss: 0.3563 - val_acc: 0.8693\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.3333 - acc: 0.8792 - val_loss: 0.3409 - val_acc: 0.8763\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.3193 - acc: 0.8833 - val_loss: 0.3503 - val_acc: 0.8712\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.3060 - acc: 0.8886 - val_loss: 0.3420 - val_acc: 0.8762\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.2960 - acc: 0.8908 - val_loss: 0.3303 - val_acc: 0.8783\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2856 - acc: 0.8951 - val_loss: 0.3157 - val_acc: 0.8832\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2753 - acc: 0.8989 - val_loss: 0.3233 - val_acc: 0.8807\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2668 - acc: 0.9018 - val_loss: 0.3236 - val_acc: 0.8787\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.2597 - acc: 0.9055 - val_loss: 0.3268 - val_acc: 0.8797\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.2506 - acc: 0.9087 - val_loss: 0.3202 - val_acc: 0.8818\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.2448 - acc: 0.9091 - val_loss: 0.2964 - val_acc: 0.8910\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.2335 - acc: 0.9139 - val_loss: 0.3012 - val_acc: 0.8898\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.2285 - acc: 0.9169 - val_loss: 0.2948 - val_acc: 0.8923\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.2221 - acc: 0.9193 - val_loss: 0.2947 - val_acc: 0.8942\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.2171 - acc: 0.9196 - val_loss: 0.2936 - val_acc: 0.8940\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.2112 - acc: 0.9226 - val_loss: 0.2939 - val_acc: 0.8920\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.2056 - acc: 0.9245 - val_loss: 0.2925 - val_acc: 0.8935\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1988 - acc: 0.9271 - val_loss: 0.2935 - val_acc: 0.8943\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.1960 - acc: 0.9282 - val_loss: 0.3038 - val_acc: 0.8875\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1880 - acc: 0.9319 - val_loss: 0.2904 - val_acc: 0.8992\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1826 - acc: 0.9343 - val_loss: 0.2934 - val_acc: 0.8978\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1793 - acc: 0.9346 - val_loss: 0.2979 - val_acc: 0.8912\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1739 - acc: 0.9374 - val_loss: 0.2929 - val_acc: 0.8932\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1697 - acc: 0.9384 - val_loss: 0.2884 - val_acc: 0.8970\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1650 - acc: 0.9411 - val_loss: 0.2869 - val_acc: 0.8957\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1603 - acc: 0.9419 - val_loss: 0.2864 - val_acc: 0.8953\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1565 - acc: 0.9429 - val_loss: 0.2881 - val_acc: 0.8983\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1521 - acc: 0.9453 - val_loss: 0.3080 - val_acc: 0.8912\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1468 - acc: 0.9479 - val_loss: 0.2874 - val_acc: 0.8995\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1437 - acc: 0.9486 - val_loss: 0.2918 - val_acc: 0.8952\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1404 - acc: 0.9500 - val_loss: 0.2960 - val_acc: 0.8947\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1366 - acc: 0.9514 - val_loss: 0.2951 - val_acc: 0.8940\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1324 - acc: 0.9528 - val_loss: 0.3061 - val_acc: 0.8930\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1300 - acc: 0.9524 - val_loss: 0.3146 - val_acc: 0.8962\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1258 - acc: 0.9553 - val_loss: 0.3032 - val_acc: 0.8977\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1223 - acc: 0.9570 - val_loss: 0.3008 - val_acc: 0.8953\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1218 - acc: 0.9566 - val_loss: 0.3198 - val_acc: 0.8898\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1144 - acc: 0.9591 - val_loss: 0.3171 - val_acc: 0.8902\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1122 - acc: 0.9609 - val_loss: 0.2991 - val_acc: 0.8983\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.1086 - acc: 0.9616 - val_loss: 0.3037 - val_acc: 0.8953\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1067 - acc: 0.9625 - val_loss: 0.3061 - val_acc: 0.8962\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1017 - acc: 0.9640 - val_loss: 0.3030 - val_acc: 0.8962\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1010 - acc: 0.9656 - val_loss: 0.3207 - val_acc: 0.8932\n",
      "Epoch 48/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.0966 - acc: 0.9663 - val_loss: 0.3085 - val_acc: 0.8970\n",
      "Epoch 49/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.0949 - acc: 0.9675 - val_loss: 0.3165 - val_acc: 0.8967\n",
      "Epoch 50/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.0934 - acc: 0.9679 - val_loss: 0.3217 - val_acc: 0.8937\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.3131 - acc: 0.8915\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.6003 - acc: 0.7957 - val_loss: 0.4517 - val_acc: 0.8388\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.4305 - acc: 0.8482 - val_loss: 0.4130 - val_acc: 0.8497\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.3944 - acc: 0.8585 - val_loss: 0.3938 - val_acc: 0.8573\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.3688 - acc: 0.8689 - val_loss: 0.3652 - val_acc: 0.8675\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3496 - acc: 0.8738 - val_loss: 0.3495 - val_acc: 0.8700\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.3336 - acc: 0.8786 - val_loss: 0.3388 - val_acc: 0.8757\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.3193 - acc: 0.8837 - val_loss: 0.3394 - val_acc: 0.8773\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.3077 - acc: 0.8876 - val_loss: 0.3316 - val_acc: 0.8752\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.2985 - acc: 0.8917 - val_loss: 0.3300 - val_acc: 0.8802\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.2844 - acc: 0.8956 - val_loss: 0.3187 - val_acc: 0.8837\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2757 - acc: 0.8985 - val_loss: 0.3140 - val_acc: 0.8842\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.2661 - acc: 0.9022 - val_loss: 0.3113 - val_acc: 0.8892\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2608 - acc: 0.9043 - val_loss: 0.3078 - val_acc: 0.8895\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.2526 - acc: 0.9074 - val_loss: 0.3018 - val_acc: 0.8907\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.2431 - acc: 0.9102 - val_loss: 0.2975 - val_acc: 0.8925\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2369 - acc: 0.9131 - val_loss: 0.3050 - val_acc: 0.8908\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.2304 - acc: 0.9149 - val_loss: 0.2995 - val_acc: 0.8922\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.2243 - acc: 0.9180 - val_loss: 0.2964 - val_acc: 0.8913\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.2179 - acc: 0.9201 - val_loss: 0.2965 - val_acc: 0.8945\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.2096 - acc: 0.9230 - val_loss: 0.3092 - val_acc: 0.8857\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2065 - acc: 0.9245 - val_loss: 0.2871 - val_acc: 0.8930\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2006 - acc: 0.9273 - val_loss: 0.2904 - val_acc: 0.8930\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1954 - acc: 0.9285 - val_loss: 0.2891 - val_acc: 0.8977\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1910 - acc: 0.9307 - val_loss: 0.2863 - val_acc: 0.8942\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1841 - acc: 0.9333 - val_loss: 0.2962 - val_acc: 0.8923\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1787 - acc: 0.9358 - val_loss: 0.2914 - val_acc: 0.8938\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.1757 - acc: 0.9361 - val_loss: 0.2895 - val_acc: 0.8985\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1696 - acc: 0.9387 - val_loss: 0.2929 - val_acc: 0.8923\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1658 - acc: 0.9404 - val_loss: 0.2888 - val_acc: 0.8950\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1598 - acc: 0.9429 - val_loss: 0.2954 - val_acc: 0.8995\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.1575 - acc: 0.9434 - val_loss: 0.3056 - val_acc: 0.8907\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1537 - acc: 0.9448 - val_loss: 0.2984 - val_acc: 0.8955\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1488 - acc: 0.9461 - val_loss: 0.3017 - val_acc: 0.8942\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1447 - acc: 0.9479 - val_loss: 0.2942 - val_acc: 0.8958\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1444 - acc: 0.9486 - val_loss: 0.2897 - val_acc: 0.9020\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1376 - acc: 0.9516 - val_loss: 0.2862 - val_acc: 0.9033\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1337 - acc: 0.9525 - val_loss: 0.2912 - val_acc: 0.9025\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1317 - acc: 0.9539 - val_loss: 0.2947 - val_acc: 0.8993\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1264 - acc: 0.9559 - val_loss: 0.3128 - val_acc: 0.8947\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1248 - acc: 0.9561 - val_loss: 0.3232 - val_acc: 0.8918\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1233 - acc: 0.9562 - val_loss: 0.3008 - val_acc: 0.9022\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.1158 - acc: 0.9591 - val_loss: 0.3084 - val_acc: 0.8982\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1132 - acc: 0.9605 - val_loss: 0.3052 - val_acc: 0.8980\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1111 - acc: 0.9619 - val_loss: 0.3126 - val_acc: 0.8973\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1087 - acc: 0.9624 - val_loss: 0.3192 - val_acc: 0.8965\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1047 - acc: 0.9642 - val_loss: 0.3080 - val_acc: 0.8970\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1029 - acc: 0.9641 - val_loss: 0.3051 - val_acc: 0.8995\n",
      "Epoch 48/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0995 - acc: 0.9666 - val_loss: 0.3196 - val_acc: 0.8995\n",
      "Epoch 49/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.0971 - acc: 0.9665 - val_loss: 0.3289 - val_acc: 0.8963\n",
      "Epoch 50/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.0927 - acc: 0.9687 - val_loss: 0.3161 - val_acc: 0.9007\n",
      "Epoch 51/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.0903 - acc: 0.9698 - val_loss: 0.3233 - val_acc: 0.8978\n",
      "Epoch 52/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.0878 - acc: 0.9705 - val_loss: 0.3249 - val_acc: 0.9012\n",
      "Epoch 53/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0849 - acc: 0.9719 - val_loss: 0.3244 - val_acc: 0.8995\n",
      "Epoch 54/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0848 - acc: 0.9711 - val_loss: 0.3163 - val_acc: 0.9018\n",
      "Epoch 55/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.0830 - acc: 0.9718 - val_loss: 0.3249 - val_acc: 0.9012\n",
      "Epoch 56/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0821 - acc: 0.9718 - val_loss: 0.3589 - val_acc: 0.8953\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.3181 - acc: 0.8940\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.6128 - acc: 0.7926 - val_loss: 0.4713 - val_acc: 0.8327\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.4306 - acc: 0.8471 - val_loss: 0.4128 - val_acc: 0.8538\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3948 - acc: 0.8598 - val_loss: 0.3949 - val_acc: 0.8535\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3705 - acc: 0.8668 - val_loss: 0.3749 - val_acc: 0.8685\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3491 - acc: 0.8744 - val_loss: 0.3630 - val_acc: 0.8667\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3337 - acc: 0.8793 - val_loss: 0.3427 - val_acc: 0.8772\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3224 - acc: 0.8833 - val_loss: 0.3375 - val_acc: 0.8805\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3083 - acc: 0.8883 - val_loss: 0.3298 - val_acc: 0.8783\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2956 - acc: 0.8913 - val_loss: 0.3431 - val_acc: 0.8743\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2859 - acc: 0.8959 - val_loss: 0.3218 - val_acc: 0.8843\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2771 - acc: 0.8983 - val_loss: 0.3131 - val_acc: 0.8890\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2693 - acc: 0.9009 - val_loss: 0.3148 - val_acc: 0.8865\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2607 - acc: 0.9036 - val_loss: 0.3062 - val_acc: 0.8908\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2517 - acc: 0.9065 - val_loss: 0.3044 - val_acc: 0.8907\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2435 - acc: 0.9108 - val_loss: 0.3005 - val_acc: 0.8933\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2358 - acc: 0.9125 - val_loss: 0.3140 - val_acc: 0.8837\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2310 - acc: 0.9143 - val_loss: 0.3022 - val_acc: 0.8905\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2238 - acc: 0.9174 - val_loss: 0.2969 - val_acc: 0.8937\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2172 - acc: 0.9202 - val_loss: 0.2940 - val_acc: 0.8932\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2125 - acc: 0.9208 - val_loss: 0.2941 - val_acc: 0.8918\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2058 - acc: 0.9239 - val_loss: 0.2911 - val_acc: 0.8942\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2000 - acc: 0.9272 - val_loss: 0.2977 - val_acc: 0.8952\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1948 - acc: 0.9282 - val_loss: 0.2853 - val_acc: 0.8960\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1898 - acc: 0.9302 - val_loss: 0.2856 - val_acc: 0.8973\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1840 - acc: 0.9335 - val_loss: 0.2929 - val_acc: 0.8952\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1806 - acc: 0.9345 - val_loss: 0.2986 - val_acc: 0.8978\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1747 - acc: 0.9365 - val_loss: 0.3116 - val_acc: 0.8895\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1706 - acc: 0.9378 - val_loss: 0.3056 - val_acc: 0.8917\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1663 - acc: 0.9399 - val_loss: 0.3020 - val_acc: 0.8927\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1621 - acc: 0.9409 - val_loss: 0.2895 - val_acc: 0.8987\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1573 - acc: 0.9422 - val_loss: 0.2941 - val_acc: 0.8988\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1534 - acc: 0.9452 - val_loss: 0.2977 - val_acc: 0.8995\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1499 - acc: 0.9464 - val_loss: 0.2986 - val_acc: 0.8957\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1464 - acc: 0.9470 - val_loss: 0.3014 - val_acc: 0.8930\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1424 - acc: 0.9490 - val_loss: 0.2942 - val_acc: 0.9007\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1397 - acc: 0.9503 - val_loss: 0.2963 - val_acc: 0.8997\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1331 - acc: 0.9533 - val_loss: 0.2988 - val_acc: 0.8993\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1298 - acc: 0.9535 - val_loss: 0.2932 - val_acc: 0.9023\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1270 - acc: 0.9546 - val_loss: 0.3009 - val_acc: 0.8968\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1247 - acc: 0.9556 - val_loss: 0.3018 - val_acc: 0.9000\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1213 - acc: 0.9569 - val_loss: 0.3000 - val_acc: 0.9015\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1163 - acc: 0.9596 - val_loss: 0.3086 - val_acc: 0.9008\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1145 - acc: 0.9599 - val_loss: 0.3054 - val_acc: 0.8990\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.3156 - acc: 0.8889\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.6109 - acc: 0.7933 - val_loss: 0.4744 - val_acc: 0.8310\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.4327 - acc: 0.8467 - val_loss: 0.4117 - val_acc: 0.8482\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3949 - acc: 0.8592 - val_loss: 0.3874 - val_acc: 0.8567\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3704 - acc: 0.8666 - val_loss: 0.3697 - val_acc: 0.8642\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.3498 - acc: 0.8748 - val_loss: 0.3478 - val_acc: 0.8733\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3356 - acc: 0.8787 - val_loss: 0.3378 - val_acc: 0.8773\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3228 - acc: 0.8828 - val_loss: 0.3315 - val_acc: 0.8778\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3074 - acc: 0.8882 - val_loss: 0.3256 - val_acc: 0.8825\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2973 - acc: 0.8918 - val_loss: 0.3170 - val_acc: 0.8832\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2875 - acc: 0.8947 - val_loss: 0.3180 - val_acc: 0.8848\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2780 - acc: 0.8981 - val_loss: 0.3108 - val_acc: 0.8885\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2679 - acc: 0.9021 - val_loss: 0.3135 - val_acc: 0.8863\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2600 - acc: 0.9046 - val_loss: 0.3139 - val_acc: 0.8870\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2509 - acc: 0.9073 - val_loss: 0.3280 - val_acc: 0.8793\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2447 - acc: 0.9091 - val_loss: 0.3042 - val_acc: 0.8888\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2380 - acc: 0.9127 - val_loss: 0.2978 - val_acc: 0.8917\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2315 - acc: 0.9141 - val_loss: 0.3029 - val_acc: 0.8877\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2241 - acc: 0.9179 - val_loss: 0.2940 - val_acc: 0.8963\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2193 - acc: 0.9192 - val_loss: 0.2904 - val_acc: 0.8943\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2122 - acc: 0.9217 - val_loss: 0.3122 - val_acc: 0.8853\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2049 - acc: 0.9248 - val_loss: 0.3036 - val_acc: 0.8863\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2018 - acc: 0.9267 - val_loss: 0.2973 - val_acc: 0.8907\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1955 - acc: 0.9294 - val_loss: 0.2886 - val_acc: 0.8975\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1900 - acc: 0.9319 - val_loss: 0.3009 - val_acc: 0.8882\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1863 - acc: 0.9319 - val_loss: 0.2874 - val_acc: 0.8970\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1817 - acc: 0.9341 - val_loss: 0.2937 - val_acc: 0.8932\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1775 - acc: 0.9361 - val_loss: 0.2892 - val_acc: 0.8942\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1703 - acc: 0.9381 - val_loss: 0.2857 - val_acc: 0.8983\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1659 - acc: 0.9401 - val_loss: 0.2898 - val_acc: 0.8955\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1629 - acc: 0.9409 - val_loss: 0.2882 - val_acc: 0.8965\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1573 - acc: 0.9431 - val_loss: 0.2944 - val_acc: 0.8933\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1556 - acc: 0.9438 - val_loss: 0.2945 - val_acc: 0.8953\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1505 - acc: 0.9456 - val_loss: 0.2897 - val_acc: 0.8965\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1453 - acc: 0.9480 - val_loss: 0.2929 - val_acc: 0.8968\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1430 - acc: 0.9490 - val_loss: 0.2936 - val_acc: 0.8963\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1384 - acc: 0.9505 - val_loss: 0.3147 - val_acc: 0.8900\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1335 - acc: 0.9530 - val_loss: 0.3094 - val_acc: 0.8925\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1311 - acc: 0.9534 - val_loss: 0.2911 - val_acc: 0.8985\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1281 - acc: 0.9546 - val_loss: 0.2998 - val_acc: 0.8965\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1242 - acc: 0.9571 - val_loss: 0.3056 - val_acc: 0.8967\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1202 - acc: 0.9586 - val_loss: 0.2948 - val_acc: 0.8975\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1169 - acc: 0.9588 - val_loss: 0.3129 - val_acc: 0.8948\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1134 - acc: 0.9600 - val_loss: 0.3046 - val_acc: 0.8973\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1118 - acc: 0.9606 - val_loss: 0.3059 - val_acc: 0.8937\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1074 - acc: 0.9627 - val_loss: 0.3064 - val_acc: 0.8990\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1049 - acc: 0.9640 - val_loss: 0.3046 - val_acc: 0.8978\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1022 - acc: 0.9643 - val_loss: 0.3074 - val_acc: 0.8963\n",
      "Epoch 48/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.0990 - acc: 0.9653 - val_loss: 0.3270 - val_acc: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.3152 - acc: 0.8905\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 49us/sample - loss: 0.6080 - acc: 0.7946 - val_loss: 0.4711 - val_acc: 0.8360\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.4324 - acc: 0.8462 - val_loss: 0.4068 - val_acc: 0.8530\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.3933 - acc: 0.8602 - val_loss: 0.4021 - val_acc: 0.8498\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3680 - acc: 0.8689 - val_loss: 0.3833 - val_acc: 0.8610\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3503 - acc: 0.8739 - val_loss: 0.3499 - val_acc: 0.8708\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3339 - acc: 0.8794 - val_loss: 0.3363 - val_acc: 0.8820\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3205 - acc: 0.8836 - val_loss: 0.3271 - val_acc: 0.8810\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3071 - acc: 0.8876 - val_loss: 0.3228 - val_acc: 0.8820\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2963 - acc: 0.8920 - val_loss: 0.3238 - val_acc: 0.8820\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2863 - acc: 0.8954 - val_loss: 0.3163 - val_acc: 0.8843\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2761 - acc: 0.8987 - val_loss: 0.3146 - val_acc: 0.8850\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2675 - acc: 0.9008 - val_loss: 0.3075 - val_acc: 0.8877\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2589 - acc: 0.9048 - val_loss: 0.3049 - val_acc: 0.8887\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2502 - acc: 0.9088 - val_loss: 0.3114 - val_acc: 0.8875\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2430 - acc: 0.9106 - val_loss: 0.3072 - val_acc: 0.8877\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2368 - acc: 0.9127 - val_loss: 0.2959 - val_acc: 0.8915\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2291 - acc: 0.9152 - val_loss: 0.2964 - val_acc: 0.8922\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2223 - acc: 0.9187 - val_loss: 0.3040 - val_acc: 0.8882\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2170 - acc: 0.9209 - val_loss: 0.2910 - val_acc: 0.8938\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2114 - acc: 0.9228 - val_loss: 0.2933 - val_acc: 0.8968\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2048 - acc: 0.9247 - val_loss: 0.2899 - val_acc: 0.8942\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2015 - acc: 0.9270 - val_loss: 0.2917 - val_acc: 0.8953\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1945 - acc: 0.9292 - val_loss: 0.3021 - val_acc: 0.8925\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1887 - acc: 0.9311 - val_loss: 0.2991 - val_acc: 0.8932\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1842 - acc: 0.9330 - val_loss: 0.2861 - val_acc: 0.8952\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1815 - acc: 0.9342 - val_loss: 0.2883 - val_acc: 0.8947\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1748 - acc: 0.9376 - val_loss: 0.2917 - val_acc: 0.8933\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.1707 - acc: 0.9387 - val_loss: 0.2870 - val_acc: 0.8985\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1644 - acc: 0.9406 - val_loss: 0.2871 - val_acc: 0.8975\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1611 - acc: 0.9421 - val_loss: 0.2911 - val_acc: 0.8987\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1585 - acc: 0.9429 - val_loss: 0.3051 - val_acc: 0.8947\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1538 - acc: 0.9445 - val_loss: 0.2965 - val_acc: 0.8938\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1498 - acc: 0.9464 - val_loss: 0.2969 - val_acc: 0.8975\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1459 - acc: 0.9477 - val_loss: 0.2982 - val_acc: 0.8975\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1405 - acc: 0.9497 - val_loss: 0.2918 - val_acc: 0.8993\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1375 - acc: 0.9514 - val_loss: 0.2962 - val_acc: 0.9018\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1349 - acc: 0.9520 - val_loss: 0.2902 - val_acc: 0.8993\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1293 - acc: 0.9540 - val_loss: 0.3058 - val_acc: 0.8980\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1272 - acc: 0.9553 - val_loss: 0.2961 - val_acc: 0.8970\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1236 - acc: 0.9562 - val_loss: 0.3035 - val_acc: 0.8978\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1197 - acc: 0.9581 - val_loss: 0.3077 - val_acc: 0.8952\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1180 - acc: 0.9583 - val_loss: 0.2997 - val_acc: 0.8998\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1133 - acc: 0.9608 - val_loss: 0.3114 - val_acc: 0.8968\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1113 - acc: 0.9614 - val_loss: 0.3160 - val_acc: 0.8932\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1063 - acc: 0.9641 - val_loss: 0.3040 - val_acc: 0.8985\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.3160 - acc: 0.8881\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.6029 - acc: 0.7946 - val_loss: 0.4627 - val_acc: 0.8345\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.4299 - acc: 0.8475 - val_loss: 0.4105 - val_acc: 0.8517\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.3944 - acc: 0.8582 - val_loss: 0.3904 - val_acc: 0.8573\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.3692 - acc: 0.8672 - val_loss: 0.3642 - val_acc: 0.8675\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3495 - acc: 0.8742 - val_loss: 0.3488 - val_acc: 0.8743\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3347 - acc: 0.8782 - val_loss: 0.3423 - val_acc: 0.8743\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3201 - acc: 0.8836 - val_loss: 0.3420 - val_acc: 0.8788\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3093 - acc: 0.8877 - val_loss: 0.3284 - val_acc: 0.8810\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2943 - acc: 0.8920 - val_loss: 0.3249 - val_acc: 0.8837\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2856 - acc: 0.8947 - val_loss: 0.3179 - val_acc: 0.8850\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2782 - acc: 0.8982 - val_loss: 0.3151 - val_acc: 0.8865\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.2661 - acc: 0.9033 - val_loss: 0.3099 - val_acc: 0.8862\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2596 - acc: 0.9042 - val_loss: 0.3078 - val_acc: 0.8878\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2523 - acc: 0.9068 - val_loss: 0.3084 - val_acc: 0.8880\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2453 - acc: 0.9096 - val_loss: 0.2986 - val_acc: 0.8925\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2371 - acc: 0.9127 - val_loss: 0.2979 - val_acc: 0.8902\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2297 - acc: 0.9161 - val_loss: 0.3006 - val_acc: 0.8920\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2238 - acc: 0.9171 - val_loss: 0.2946 - val_acc: 0.8938\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2192 - acc: 0.9197 - val_loss: 0.2925 - val_acc: 0.8942\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 3s 46us/sample - loss: 0.2124 - acc: 0.9226 - val_loss: 0.2945 - val_acc: 0.8908\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2074 - acc: 0.9237 - val_loss: 0.3100 - val_acc: 0.8855\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2028 - acc: 0.9258 - val_loss: 0.2886 - val_acc: 0.8945\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1962 - acc: 0.9284 - val_loss: 0.2967 - val_acc: 0.8908\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1901 - acc: 0.9313 - val_loss: 0.2969 - val_acc: 0.8957\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1851 - acc: 0.9329 - val_loss: 0.3053 - val_acc: 0.8883\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1820 - acc: 0.9336 - val_loss: 0.2887 - val_acc: 0.8950\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.1761 - acc: 0.9363 - val_loss: 0.2893 - val_acc: 0.8937\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1709 - acc: 0.9380 - val_loss: 0.2930 - val_acc: 0.8943\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1667 - acc: 0.9396 - val_loss: 0.2986 - val_acc: 0.8948\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.1642 - acc: 0.9406 - val_loss: 0.2891 - val_acc: 0.8957\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1589 - acc: 0.9432 - val_loss: 0.2904 - val_acc: 0.8982\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1541 - acc: 0.9444 - val_loss: 0.3110 - val_acc: 0.8922\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1516 - acc: 0.9448 - val_loss: 0.2987 - val_acc: 0.8922\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1458 - acc: 0.9479 - val_loss: 0.2921 - val_acc: 0.8963\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1435 - acc: 0.9476 - val_loss: 0.2998 - val_acc: 0.8968\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1385 - acc: 0.9505 - val_loss: 0.2978 - val_acc: 0.8938\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.1351 - acc: 0.9519 - val_loss: 0.3011 - val_acc: 0.8958\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1306 - acc: 0.9535 - val_loss: 0.3150 - val_acc: 0.8932\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.1294 - acc: 0.9544 - val_loss: 0.3036 - val_acc: 0.8957\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1260 - acc: 0.9549 - val_loss: 0.3060 - val_acc: 0.8973\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1199 - acc: 0.9571 - val_loss: 0.3000 - val_acc: 0.8982\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1179 - acc: 0.9578 - val_loss: 0.3043 - val_acc: 0.8973\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.3173 - acc: 0.8879\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6174 - acc: 0.7899 - val_loss: 0.4824 - val_acc: 0.8280\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.4348 - acc: 0.8456 - val_loss: 0.4090 - val_acc: 0.8520\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 3s 49us/sample - loss: 0.3935 - acc: 0.8599 - val_loss: 0.3900 - val_acc: 0.8598\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.3704 - acc: 0.8675 - val_loss: 0.3712 - val_acc: 0.8633\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3521 - acc: 0.8724 - val_loss: 0.3512 - val_acc: 0.8717\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3353 - acc: 0.8785 - val_loss: 0.3488 - val_acc: 0.8728\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3203 - acc: 0.8841 - val_loss: 0.3383 - val_acc: 0.8792\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.3094 - acc: 0.8874 - val_loss: 0.3215 - val_acc: 0.8840\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2955 - acc: 0.8917 - val_loss: 0.3169 - val_acc: 0.8870\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2859 - acc: 0.8946 - val_loss: 0.3203 - val_acc: 0.8842\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2757 - acc: 0.8981 - val_loss: 0.3208 - val_acc: 0.8805\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2680 - acc: 0.9016 - val_loss: 0.3068 - val_acc: 0.8892\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2590 - acc: 0.9038 - val_loss: 0.3072 - val_acc: 0.8897\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2533 - acc: 0.9061 - val_loss: 0.3090 - val_acc: 0.8878\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2422 - acc: 0.9115 - val_loss: 0.3181 - val_acc: 0.8835\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2376 - acc: 0.9131 - val_loss: 0.3037 - val_acc: 0.8888\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2290 - acc: 0.9151 - val_loss: 0.3108 - val_acc: 0.8855\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2236 - acc: 0.9177 - val_loss: 0.3051 - val_acc: 0.8897\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2186 - acc: 0.9189 - val_loss: 0.3015 - val_acc: 0.8923\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2110 - acc: 0.9227 - val_loss: 0.3128 - val_acc: 0.8873\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2063 - acc: 0.9249 - val_loss: 0.2921 - val_acc: 0.8917\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1986 - acc: 0.9282 - val_loss: 0.2975 - val_acc: 0.8927\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1951 - acc: 0.9286 - val_loss: 0.2890 - val_acc: 0.8957\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1879 - acc: 0.9310 - val_loss: 0.2865 - val_acc: 0.8952\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1841 - acc: 0.9331 - val_loss: 0.2857 - val_acc: 0.8983\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1799 - acc: 0.9341 - val_loss: 0.2910 - val_acc: 0.8953\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1747 - acc: 0.9360 - val_loss: 0.2850 - val_acc: 0.8963\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1704 - acc: 0.9388 - val_loss: 0.3009 - val_acc: 0.8905\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1647 - acc: 0.9404 - val_loss: 0.2949 - val_acc: 0.8967\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1613 - acc: 0.9421 - val_loss: 0.3130 - val_acc: 0.8907\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1556 - acc: 0.9441 - val_loss: 0.2944 - val_acc: 0.8938\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1517 - acc: 0.9461 - val_loss: 0.2921 - val_acc: 0.8972\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1477 - acc: 0.9474 - val_loss: 0.2990 - val_acc: 0.8970\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1441 - acc: 0.9486 - val_loss: 0.2931 - val_acc: 0.8978\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1391 - acc: 0.9503 - val_loss: 0.2939 - val_acc: 0.9007\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1355 - acc: 0.9517 - val_loss: 0.2990 - val_acc: 0.8988\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1320 - acc: 0.9536 - val_loss: 0.2961 - val_acc: 0.8977\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1278 - acc: 0.9542 - val_loss: 0.3020 - val_acc: 0.8968\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1231 - acc: 0.9574 - val_loss: 0.3073 - val_acc: 0.8960\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1220 - acc: 0.9572 - val_loss: 0.2965 - val_acc: 0.8962\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1185 - acc: 0.9577 - val_loss: 0.3025 - val_acc: 0.8975\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1154 - acc: 0.9596 - val_loss: 0.3096 - val_acc: 0.8953\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1121 - acc: 0.9609 - val_loss: 0.3142 - val_acc: 0.8975\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1091 - acc: 0.9624 - val_loss: 0.3121 - val_acc: 0.8962\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1056 - acc: 0.9639 - val_loss: 0.3142 - val_acc: 0.9022\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1031 - acc: 0.9641 - val_loss: 0.3122 - val_acc: 0.8990\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.0996 - acc: 0.9664 - val_loss: 0.3204 - val_acc: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.3131 - acc: 0.8928\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.6054 - acc: 0.7961 - val_loss: 0.4545 - val_acc: 0.8382\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.4302 - acc: 0.8475 - val_loss: 0.4064 - val_acc: 0.8513\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3920 - acc: 0.8595 - val_loss: 0.3761 - val_acc: 0.8623\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3690 - acc: 0.8674 - val_loss: 0.3655 - val_acc: 0.8675\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3482 - acc: 0.8741 - val_loss: 0.3567 - val_acc: 0.8712\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3323 - acc: 0.8802 - val_loss: 0.3463 - val_acc: 0.8727\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3197 - acc: 0.8833 - val_loss: 0.3348 - val_acc: 0.8828\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3057 - acc: 0.8881 - val_loss: 0.3258 - val_acc: 0.8853\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2948 - acc: 0.8927 - val_loss: 0.3375 - val_acc: 0.8773\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2871 - acc: 0.8942 - val_loss: 0.3231 - val_acc: 0.8838\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2744 - acc: 0.8996 - val_loss: 0.3228 - val_acc: 0.8837\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2678 - acc: 0.9014 - val_loss: 0.3098 - val_acc: 0.8888\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2605 - acc: 0.9047 - val_loss: 0.3087 - val_acc: 0.8917\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2504 - acc: 0.9080 - val_loss: 0.2987 - val_acc: 0.8933\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2435 - acc: 0.9099 - val_loss: 0.3003 - val_acc: 0.8895\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2370 - acc: 0.9122 - val_loss: 0.3166 - val_acc: 0.8867\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2299 - acc: 0.9160 - val_loss: 0.2911 - val_acc: 0.8955\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2235 - acc: 0.9184 - val_loss: 0.2961 - val_acc: 0.8978\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2160 - acc: 0.9208 - val_loss: 0.2995 - val_acc: 0.8883\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2107 - acc: 0.9226 - val_loss: 0.3261 - val_acc: 0.8818\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.2050 - acc: 0.9255 - val_loss: 0.2876 - val_acc: 0.8968\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1982 - acc: 0.9283 - val_loss: 0.2994 - val_acc: 0.8917\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1945 - acc: 0.9295 - val_loss: 0.2907 - val_acc: 0.8927\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.1893 - acc: 0.9302 - val_loss: 0.2957 - val_acc: 0.8928\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1840 - acc: 0.9329 - val_loss: 0.2885 - val_acc: 0.8983\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1795 - acc: 0.9347 - val_loss: 0.2867 - val_acc: 0.8978\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1731 - acc: 0.9374 - val_loss: 0.2873 - val_acc: 0.8957\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1701 - acc: 0.9387 - val_loss: 0.3043 - val_acc: 0.8942\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1675 - acc: 0.9387 - val_loss: 0.2914 - val_acc: 0.8968\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1606 - acc: 0.9429 - val_loss: 0.2971 - val_acc: 0.8955\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1574 - acc: 0.9435 - val_loss: 0.2914 - val_acc: 0.8987\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1545 - acc: 0.9433 - val_loss: 0.2962 - val_acc: 0.8987\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1497 - acc: 0.9459 - val_loss: 0.3157 - val_acc: 0.8868\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1455 - acc: 0.9474 - val_loss: 0.3216 - val_acc: 0.8878\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1392 - acc: 0.9513 - val_loss: 0.3121 - val_acc: 0.8935\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1364 - acc: 0.9510 - val_loss: 0.3042 - val_acc: 0.8980\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1320 - acc: 0.9537 - val_loss: 0.2929 - val_acc: 0.8988\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1297 - acc: 0.9543 - val_loss: 0.2996 - val_acc: 0.8998\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1252 - acc: 0.9560 - val_loss: 0.3040 - val_acc: 0.8980\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1243 - acc: 0.9555 - val_loss: 0.3050 - val_acc: 0.8978\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1198 - acc: 0.9576 - val_loss: 0.3230 - val_acc: 0.8935\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1157 - acc: 0.9595 - val_loss: 0.3060 - val_acc: 0.8988\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1140 - acc: 0.9598 - val_loss: 0.3150 - val_acc: 0.8963\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1093 - acc: 0.9619 - val_loss: 0.3226 - val_acc: 0.8930\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1077 - acc: 0.9632 - val_loss: 0.3273 - val_acc: 0.8957\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1034 - acc: 0.9639 - val_loss: 0.3296 - val_acc: 0.8937\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.3131 - acc: 0.8886\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 46us/sample - loss: 0.6116 - acc: 0.7929 - val_loss: 0.4548 - val_acc: 0.8377\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.4304 - acc: 0.8471 - val_loss: 0.4195 - val_acc: 0.8477\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3933 - acc: 0.8598 - val_loss: 0.3954 - val_acc: 0.8560\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3679 - acc: 0.8682 - val_loss: 0.3651 - val_acc: 0.8690\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3505 - acc: 0.8745 - val_loss: 0.3485 - val_acc: 0.8735\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3351 - acc: 0.8792 - val_loss: 0.3405 - val_acc: 0.8750\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.3192 - acc: 0.8839 - val_loss: 0.3434 - val_acc: 0.8725\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.3084 - acc: 0.8874 - val_loss: 0.3266 - val_acc: 0.8803\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2965 - acc: 0.8919 - val_loss: 0.3316 - val_acc: 0.8805\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2857 - acc: 0.8969 - val_loss: 0.3127 - val_acc: 0.8848\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2768 - acc: 0.8985 - val_loss: 0.3118 - val_acc: 0.8863\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2669 - acc: 0.9023 - val_loss: 0.3106 - val_acc: 0.8853\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2605 - acc: 0.9037 - val_loss: 0.3053 - val_acc: 0.8878\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2504 - acc: 0.9088 - val_loss: 0.3093 - val_acc: 0.8872\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2444 - acc: 0.9094 - val_loss: 0.3057 - val_acc: 0.8873\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2370 - acc: 0.9134 - val_loss: 0.2968 - val_acc: 0.8930\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2308 - acc: 0.9162 - val_loss: 0.2939 - val_acc: 0.8933\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2237 - acc: 0.9183 - val_loss: 0.3009 - val_acc: 0.8918\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2179 - acc: 0.9195 - val_loss: 0.2932 - val_acc: 0.8937\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2109 - acc: 0.9227 - val_loss: 0.2956 - val_acc: 0.8920\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2093 - acc: 0.9234 - val_loss: 0.2986 - val_acc: 0.8895\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2025 - acc: 0.9257 - val_loss: 0.3095 - val_acc: 0.8850\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1962 - acc: 0.9288 - val_loss: 0.3019 - val_acc: 0.8913\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1896 - acc: 0.9311 - val_loss: 0.3026 - val_acc: 0.8922\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1868 - acc: 0.9319 - val_loss: 0.2931 - val_acc: 0.8918\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1796 - acc: 0.9349 - val_loss: 0.2954 - val_acc: 0.8958\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1754 - acc: 0.9369 - val_loss: 0.2957 - val_acc: 0.8937\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1722 - acc: 0.9378 - val_loss: 0.3080 - val_acc: 0.8930\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1678 - acc: 0.9392 - val_loss: 0.2941 - val_acc: 0.8963\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1640 - acc: 0.9414 - val_loss: 0.2993 - val_acc: 0.8930\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1574 - acc: 0.9432 - val_loss: 0.3079 - val_acc: 0.8925\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1564 - acc: 0.9435 - val_loss: 0.3008 - val_acc: 0.8938\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1510 - acc: 0.9464 - val_loss: 0.3031 - val_acc: 0.8943\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1439 - acc: 0.9495 - val_loss: 0.3052 - val_acc: 0.8907\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1437 - acc: 0.9482 - val_loss: 0.3033 - val_acc: 0.8937\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1375 - acc: 0.9517 - val_loss: 0.3063 - val_acc: 0.8963\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1353 - acc: 0.9523 - val_loss: 0.3056 - val_acc: 0.8952\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1302 - acc: 0.9542 - val_loss: 0.3045 - val_acc: 0.8987\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1281 - acc: 0.9537 - val_loss: 0.3136 - val_acc: 0.8947\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1242 - acc: 0.9558 - val_loss: 0.3201 - val_acc: 0.8932\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1208 - acc: 0.9576 - val_loss: 0.3215 - val_acc: 0.8938\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1166 - acc: 0.9596 - val_loss: 0.3200 - val_acc: 0.8957\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1138 - acc: 0.9602 - val_loss: 0.3110 - val_acc: 0.8950\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1114 - acc: 0.9614 - val_loss: 0.3273 - val_acc: 0.8908\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1070 - acc: 0.9630 - val_loss: 0.3276 - val_acc: 0.8938\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.3209 - acc: 0.8887\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.6270 - acc: 0.7883 - val_loss: 0.4731 - val_acc: 0.8358\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.4363 - acc: 0.8444 - val_loss: 0.4261 - val_acc: 0.8465\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3954 - acc: 0.8591 - val_loss: 0.3833 - val_acc: 0.8613\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3718 - acc: 0.8663 - val_loss: 0.3832 - val_acc: 0.8588\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3511 - acc: 0.8738 - val_loss: 0.3554 - val_acc: 0.8710\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3367 - acc: 0.8779 - val_loss: 0.3505 - val_acc: 0.8740\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.3235 - acc: 0.8832 - val_loss: 0.3399 - val_acc: 0.8725\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.3092 - acc: 0.8875 - val_loss: 0.3310 - val_acc: 0.8773\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2981 - acc: 0.8912 - val_loss: 0.3198 - val_acc: 0.8848\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2886 - acc: 0.8939 - val_loss: 0.3415 - val_acc: 0.8728\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2785 - acc: 0.8977 - val_loss: 0.3154 - val_acc: 0.8885\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2700 - acc: 0.9005 - val_loss: 0.3131 - val_acc: 0.8878\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2607 - acc: 0.9051 - val_loss: 0.3091 - val_acc: 0.8870\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2530 - acc: 0.9066 - val_loss: 0.3048 - val_acc: 0.8890\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2441 - acc: 0.9097 - val_loss: 0.3008 - val_acc: 0.8928\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2375 - acc: 0.9128 - val_loss: 0.3108 - val_acc: 0.8828\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2327 - acc: 0.9137 - val_loss: 0.3129 - val_acc: 0.8887\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2237 - acc: 0.9179 - val_loss: 0.2982 - val_acc: 0.8935\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2180 - acc: 0.9200 - val_loss: 0.3015 - val_acc: 0.8907\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2129 - acc: 0.9216 - val_loss: 0.2927 - val_acc: 0.8937\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2083 - acc: 0.9232 - val_loss: 0.2971 - val_acc: 0.8918\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2032 - acc: 0.9260 - val_loss: 0.2908 - val_acc: 0.8973\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1947 - acc: 0.9294 - val_loss: 0.2914 - val_acc: 0.8938\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1916 - acc: 0.9302 - val_loss: 0.2976 - val_acc: 0.8893\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1868 - acc: 0.9325 - val_loss: 0.2879 - val_acc: 0.8975\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1794 - acc: 0.9351 - val_loss: 0.2859 - val_acc: 0.8962\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1755 - acc: 0.9354 - val_loss: 0.2950 - val_acc: 0.8943\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1703 - acc: 0.9376 - val_loss: 0.3009 - val_acc: 0.8937\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1672 - acc: 0.9399 - val_loss: 0.3054 - val_acc: 0.8920\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1626 - acc: 0.9418 - val_loss: 0.2930 - val_acc: 0.8950\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1592 - acc: 0.9423 - val_loss: 0.3106 - val_acc: 0.8892\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1540 - acc: 0.9441 - val_loss: 0.2983 - val_acc: 0.8920\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1509 - acc: 0.9462 - val_loss: 0.2970 - val_acc: 0.8957\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1450 - acc: 0.9485 - val_loss: 0.2927 - val_acc: 0.8960\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1410 - acc: 0.9498 - val_loss: 0.3112 - val_acc: 0.8895\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1382 - acc: 0.9506 - val_loss: 0.2908 - val_acc: 0.9018\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1342 - acc: 0.9516 - val_loss: 0.2992 - val_acc: 0.8942\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1291 - acc: 0.9544 - val_loss: 0.3097 - val_acc: 0.8937\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1265 - acc: 0.9551 - val_loss: 0.3057 - val_acc: 0.8932\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1249 - acc: 0.9562 - val_loss: 0.3020 - val_acc: 0.8982\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1203 - acc: 0.9578 - val_loss: 0.3083 - val_acc: 0.8975\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1166 - acc: 0.9595 - val_loss: 0.3014 - val_acc: 0.8943\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1130 - acc: 0.9608 - val_loss: 0.3106 - val_acc: 0.8947\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1111 - acc: 0.9614 - val_loss: 0.3127 - val_acc: 0.8945\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1087 - acc: 0.9616 - val_loss: 0.3188 - val_acc: 0.8947\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1043 - acc: 0.9635 - val_loss: 0.3104 - val_acc: 0.8977\n",
      "10000/10000 [==============================] - 0s 50us/sample - loss: 0.3132 - acc: 0.8920\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6093 - acc: 0.7954 - val_loss: 0.4556 - val_acc: 0.8358\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.4314 - acc: 0.8467 - val_loss: 0.4144 - val_acc: 0.8512\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3942 - acc: 0.8600 - val_loss: 0.3801 - val_acc: 0.8578\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3687 - acc: 0.8673 - val_loss: 0.3606 - val_acc: 0.8642\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3503 - acc: 0.8744 - val_loss: 0.3656 - val_acc: 0.8643\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3348 - acc: 0.8787 - val_loss: 0.3530 - val_acc: 0.8738\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3201 - acc: 0.8844 - val_loss: 0.3342 - val_acc: 0.8762\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3070 - acc: 0.8886 - val_loss: 0.3267 - val_acc: 0.8817\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2985 - acc: 0.8910 - val_loss: 0.3283 - val_acc: 0.8805\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2880 - acc: 0.8943 - val_loss: 0.3147 - val_acc: 0.8858\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2765 - acc: 0.8986 - val_loss: 0.3141 - val_acc: 0.8842\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2688 - acc: 0.9019 - val_loss: 0.3074 - val_acc: 0.8905\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2602 - acc: 0.9041 - val_loss: 0.3044 - val_acc: 0.8858\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2527 - acc: 0.9073 - val_loss: 0.3188 - val_acc: 0.8857\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2440 - acc: 0.9109 - val_loss: 0.3059 - val_acc: 0.8917\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2362 - acc: 0.9132 - val_loss: 0.3141 - val_acc: 0.8837\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2305 - acc: 0.9155 - val_loss: 0.2994 - val_acc: 0.8925\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.2240 - acc: 0.9179 - val_loss: 0.2923 - val_acc: 0.8918\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2170 - acc: 0.9204 - val_loss: 0.2917 - val_acc: 0.8913\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2110 - acc: 0.9233 - val_loss: 0.2978 - val_acc: 0.8887\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2065 - acc: 0.9233 - val_loss: 0.2924 - val_acc: 0.8927\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2010 - acc: 0.9264 - val_loss: 0.2903 - val_acc: 0.8958\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1946 - acc: 0.9288 - val_loss: 0.3014 - val_acc: 0.8888\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1898 - acc: 0.9309 - val_loss: 0.2858 - val_acc: 0.8968\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1862 - acc: 0.9330 - val_loss: 0.2886 - val_acc: 0.8980\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1807 - acc: 0.9340 - val_loss: 0.2963 - val_acc: 0.8937\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1744 - acc: 0.9379 - val_loss: 0.2884 - val_acc: 0.8948\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1697 - acc: 0.9384 - val_loss: 0.2964 - val_acc: 0.8937\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1657 - acc: 0.9405 - val_loss: 0.2955 - val_acc: 0.8957\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1621 - acc: 0.9414 - val_loss: 0.2891 - val_acc: 0.8973\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1576 - acc: 0.9438 - val_loss: 0.3008 - val_acc: 0.8955\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1546 - acc: 0.9443 - val_loss: 0.2954 - val_acc: 0.8972\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1488 - acc: 0.9466 - val_loss: 0.3025 - val_acc: 0.8905\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1443 - acc: 0.9492 - val_loss: 0.2891 - val_acc: 0.8980\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.1415 - acc: 0.9490 - val_loss: 0.2986 - val_acc: 0.8978\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1366 - acc: 0.9506 - val_loss: 0.2912 - val_acc: 0.8978\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1340 - acc: 0.9530 - val_loss: 0.3005 - val_acc: 0.8953\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1311 - acc: 0.9534 - val_loss: 0.2969 - val_acc: 0.8982\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1255 - acc: 0.9556 - val_loss: 0.2983 - val_acc: 0.8987\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1243 - acc: 0.9558 - val_loss: 0.3151 - val_acc: 0.8927\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1199 - acc: 0.9580 - val_loss: 0.3131 - val_acc: 0.8955\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1168 - acc: 0.9588 - val_loss: 0.2997 - val_acc: 0.8977\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1124 - acc: 0.9607 - val_loss: 0.3075 - val_acc: 0.8952\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1109 - acc: 0.9619 - val_loss: 0.3096 - val_acc: 0.8967\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.3130 - acc: 0.8921\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 58us/sample - loss: 0.6044 - acc: 0.7955 - val_loss: 0.4566 - val_acc: 0.8377\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.4310 - acc: 0.8454 - val_loss: 0.4231 - val_acc: 0.8478\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3923 - acc: 0.8597 - val_loss: 0.3825 - val_acc: 0.8592\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3674 - acc: 0.8683 - val_loss: 0.3702 - val_acc: 0.8642\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3497 - acc: 0.8742 - val_loss: 0.3575 - val_acc: 0.8705\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3337 - acc: 0.8788 - val_loss: 0.3399 - val_acc: 0.8777\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3165 - acc: 0.8854 - val_loss: 0.3686 - val_acc: 0.8645\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3058 - acc: 0.8888 - val_loss: 0.3273 - val_acc: 0.8827\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2953 - acc: 0.8918 - val_loss: 0.3223 - val_acc: 0.8865\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2829 - acc: 0.8964 - val_loss: 0.3230 - val_acc: 0.8838\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2757 - acc: 0.8999 - val_loss: 0.3119 - val_acc: 0.8847\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2657 - acc: 0.9022 - val_loss: 0.3087 - val_acc: 0.8867\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2557 - acc: 0.9061 - val_loss: 0.3205 - val_acc: 0.8842\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2481 - acc: 0.9088 - val_loss: 0.3035 - val_acc: 0.8897\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2422 - acc: 0.9108 - val_loss: 0.3069 - val_acc: 0.8907\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2342 - acc: 0.9144 - val_loss: 0.3058 - val_acc: 0.8885\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2279 - acc: 0.9166 - val_loss: 0.2935 - val_acc: 0.8937\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2208 - acc: 0.9191 - val_loss: 0.3064 - val_acc: 0.8878\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2133 - acc: 0.9219 - val_loss: 0.2986 - val_acc: 0.8927\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2107 - acc: 0.9230 - val_loss: 0.2951 - val_acc: 0.8968\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2027 - acc: 0.9257 - val_loss: 0.2904 - val_acc: 0.8945\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1977 - acc: 0.9277 - val_loss: 0.2916 - val_acc: 0.8942\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1917 - acc: 0.9298 - val_loss: 0.2975 - val_acc: 0.8932\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1867 - acc: 0.9328 - val_loss: 0.2893 - val_acc: 0.8962\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1817 - acc: 0.9341 - val_loss: 0.2890 - val_acc: 0.8967\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1777 - acc: 0.9353 - val_loss: 0.2966 - val_acc: 0.8932\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1742 - acc: 0.9379 - val_loss: 0.2870 - val_acc: 0.9010\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1681 - acc: 0.9384 - val_loss: 0.2951 - val_acc: 0.8982\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1634 - acc: 0.9416 - val_loss: 0.2894 - val_acc: 0.8978\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1583 - acc: 0.9425 - val_loss: 0.3027 - val_acc: 0.8900\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1553 - acc: 0.9438 - val_loss: 0.2918 - val_acc: 0.9000\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1488 - acc: 0.9461 - val_loss: 0.3011 - val_acc: 0.8958\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1476 - acc: 0.9465 - val_loss: 0.2992 - val_acc: 0.8948\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1417 - acc: 0.9490 - val_loss: 0.2937 - val_acc: 0.9012\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1377 - acc: 0.9510 - val_loss: 0.2951 - val_acc: 0.8963\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1354 - acc: 0.9517 - val_loss: 0.2958 - val_acc: 0.8960\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1316 - acc: 0.9531 - val_loss: 0.2938 - val_acc: 0.8987\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1282 - acc: 0.9538 - val_loss: 0.3074 - val_acc: 0.8968\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1231 - acc: 0.9569 - val_loss: 0.2988 - val_acc: 0.8985\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1228 - acc: 0.9571 - val_loss: 0.3194 - val_acc: 0.8947\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1177 - acc: 0.9582 - val_loss: 0.3015 - val_acc: 0.8982\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1138 - acc: 0.9594 - val_loss: 0.3206 - val_acc: 0.8912\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1111 - acc: 0.9611 - val_loss: 0.3119 - val_acc: 0.8970\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1069 - acc: 0.9636 - val_loss: 0.3136 - val_acc: 0.8965\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1043 - acc: 0.9636 - val_loss: 0.2989 - val_acc: 0.9000\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1012 - acc: 0.9649 - val_loss: 0.3127 - val_acc: 0.9007\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0984 - acc: 0.9664 - val_loss: 0.3098 - val_acc: 0.9018\n",
      "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3148 - acc: 0.8922\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 4s 66us/sample - loss: 0.6035 - acc: 0.7965 - val_loss: 0.4576 - val_acc: 0.8347\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.4301 - acc: 0.8476 - val_loss: 0.4205 - val_acc: 0.8430\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.3924 - acc: 0.8606 - val_loss: 0.3838 - val_acc: 0.8580\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3694 - acc: 0.8681 - val_loss: 0.3606 - val_acc: 0.8670\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3482 - acc: 0.8744 - val_loss: 0.3541 - val_acc: 0.8743\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3317 - acc: 0.8790 - val_loss: 0.3575 - val_acc: 0.8678\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3189 - acc: 0.8854 - val_loss: 0.3366 - val_acc: 0.8762\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3051 - acc: 0.8887 - val_loss: 0.3279 - val_acc: 0.8785\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2931 - acc: 0.8928 - val_loss: 0.3183 - val_acc: 0.8867\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2846 - acc: 0.8959 - val_loss: 0.3175 - val_acc: 0.8870\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2739 - acc: 0.8993 - val_loss: 0.3200 - val_acc: 0.8818\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2648 - acc: 0.9034 - val_loss: 0.3077 - val_acc: 0.8893\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2568 - acc: 0.9064 - val_loss: 0.3020 - val_acc: 0.8877\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2487 - acc: 0.9085 - val_loss: 0.3223 - val_acc: 0.8790\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2423 - acc: 0.9115 - val_loss: 0.3121 - val_acc: 0.8852\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2357 - acc: 0.9135 - val_loss: 0.2941 - val_acc: 0.8905\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2283 - acc: 0.9151 - val_loss: 0.2903 - val_acc: 0.8923\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2221 - acc: 0.9180 - val_loss: 0.2985 - val_acc: 0.8940\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2151 - acc: 0.9219 - val_loss: 0.3070 - val_acc: 0.8877\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2107 - acc: 0.9225 - val_loss: 0.2892 - val_acc: 0.8923\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2028 - acc: 0.9257 - val_loss: 0.2976 - val_acc: 0.8950\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1997 - acc: 0.9266 - val_loss: 0.2937 - val_acc: 0.8952\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1931 - acc: 0.9297 - val_loss: 0.3033 - val_acc: 0.8908\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.1875 - acc: 0.9326 - val_loss: 0.2956 - val_acc: 0.8972\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1831 - acc: 0.9339 - val_loss: 0.2863 - val_acc: 0.8997\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1784 - acc: 0.9349 - val_loss: 0.2851 - val_acc: 0.8947\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1718 - acc: 0.9380 - val_loss: 0.2878 - val_acc: 0.8938\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1670 - acc: 0.9399 - val_loss: 0.2990 - val_acc: 0.8968\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1641 - acc: 0.9406 - val_loss: 0.2913 - val_acc: 0.8953\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1596 - acc: 0.9426 - val_loss: 0.2863 - val_acc: 0.9002\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1559 - acc: 0.9436 - val_loss: 0.2903 - val_acc: 0.8933\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1510 - acc: 0.9458 - val_loss: 0.3011 - val_acc: 0.8905\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1480 - acc: 0.9470 - val_loss: 0.2930 - val_acc: 0.8993\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1429 - acc: 0.9490 - val_loss: 0.2946 - val_acc: 0.8963\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1381 - acc: 0.9513 - val_loss: 0.2939 - val_acc: 0.8955\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1349 - acc: 0.9519 - val_loss: 0.2932 - val_acc: 0.8960\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1304 - acc: 0.9532 - val_loss: 0.2919 - val_acc: 0.8993\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1283 - acc: 0.9545 - val_loss: 0.3065 - val_acc: 0.8948\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1253 - acc: 0.9554 - val_loss: 0.2890 - val_acc: 0.9017\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1218 - acc: 0.9573 - val_loss: 0.3072 - val_acc: 0.8927\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1186 - acc: 0.9587 - val_loss: 0.2988 - val_acc: 0.9003\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1144 - acc: 0.9593 - val_loss: 0.3155 - val_acc: 0.8953\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1113 - acc: 0.9611 - val_loss: 0.3124 - val_acc: 0.8958\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1096 - acc: 0.9614 - val_loss: 0.3077 - val_acc: 0.8978\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1071 - acc: 0.9616 - val_loss: 0.3096 - val_acc: 0.8968\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1011 - acc: 0.9647 - val_loss: 0.3111 - val_acc: 0.8983\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.3158 - acc: 0.8900\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 64us/sample - loss: 0.6017 - acc: 0.7962 - val_loss: 0.4642 - val_acc: 0.8313\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.4323 - acc: 0.8460 - val_loss: 0.4109 - val_acc: 0.8488\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3931 - acc: 0.8601 - val_loss: 0.3853 - val_acc: 0.8623\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.3690 - acc: 0.8674 - val_loss: 0.3604 - val_acc: 0.8702\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.3504 - acc: 0.8739 - val_loss: 0.3678 - val_acc: 0.8667\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3325 - acc: 0.8802 - val_loss: 0.3411 - val_acc: 0.8790\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.3199 - acc: 0.8829 - val_loss: 0.3401 - val_acc: 0.8768\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.3074 - acc: 0.8885 - val_loss: 0.3341 - val_acc: 0.8813\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2961 - acc: 0.8916 - val_loss: 0.3350 - val_acc: 0.8725\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2861 - acc: 0.8947 - val_loss: 0.3341 - val_acc: 0.8753\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2754 - acc: 0.8990 - val_loss: 0.3154 - val_acc: 0.8868\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2671 - acc: 0.9024 - val_loss: 0.3062 - val_acc: 0.8925\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2587 - acc: 0.9046 - val_loss: 0.3063 - val_acc: 0.8848\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2525 - acc: 0.9074 - val_loss: 0.3041 - val_acc: 0.8843\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2433 - acc: 0.9108 - val_loss: 0.3129 - val_acc: 0.8838\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2368 - acc: 0.9140 - val_loss: 0.2981 - val_acc: 0.8887\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2307 - acc: 0.9154 - val_loss: 0.3151 - val_acc: 0.8803\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2247 - acc: 0.9175 - val_loss: 0.2973 - val_acc: 0.8898\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2219 - acc: 0.9185 - val_loss: 0.3169 - val_acc: 0.8820\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2105 - acc: 0.9227 - val_loss: 0.3045 - val_acc: 0.8855\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2068 - acc: 0.9247 - val_loss: 0.2901 - val_acc: 0.8960\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2010 - acc: 0.9268 - val_loss: 0.2919 - val_acc: 0.8955\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1954 - acc: 0.9294 - val_loss: 0.2925 - val_acc: 0.8937\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1911 - acc: 0.9293 - val_loss: 0.2918 - val_acc: 0.8930\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1838 - acc: 0.9334 - val_loss: 0.2959 - val_acc: 0.8910\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1809 - acc: 0.9341 - val_loss: 0.2915 - val_acc: 0.8942\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1757 - acc: 0.9360 - val_loss: 0.2853 - val_acc: 0.8968\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1713 - acc: 0.9376 - val_loss: 0.3000 - val_acc: 0.8933\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1667 - acc: 0.9392 - val_loss: 0.2896 - val_acc: 0.8952\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1624 - acc: 0.9404 - val_loss: 0.3012 - val_acc: 0.8923\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - ETA: 0s - loss: 0.1587 - acc: 0.942 - 2s 40us/sample - loss: 0.1587 - acc: 0.9426 - val_loss: 0.2959 - val_acc: 0.8903\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1525 - acc: 0.9452 - val_loss: 0.2891 - val_acc: 0.8972\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1507 - acc: 0.9455 - val_loss: 0.3131 - val_acc: 0.8908\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1456 - acc: 0.9488 - val_loss: 0.2916 - val_acc: 0.8993\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1418 - acc: 0.9492 - val_loss: 0.3096 - val_acc: 0.8908\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1384 - acc: 0.9502 - val_loss: 0.2907 - val_acc: 0.8958\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1336 - acc: 0.9521 - val_loss: 0.2987 - val_acc: 0.8955\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1321 - acc: 0.9528 - val_loss: 0.3033 - val_acc: 0.8942\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1281 - acc: 0.9545 - val_loss: 0.2988 - val_acc: 0.9000\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1262 - acc: 0.9560 - val_loss: 0.3012 - val_acc: 0.8960\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1229 - acc: 0.9564 - val_loss: 0.3162 - val_acc: 0.8935\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1187 - acc: 0.9577 - val_loss: 0.3282 - val_acc: 0.8887\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1139 - acc: 0.9607 - val_loss: 0.3070 - val_acc: 0.8967\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1114 - acc: 0.9610 - val_loss: 0.3047 - val_acc: 0.9012\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1088 - acc: 0.9623 - val_loss: 0.3114 - val_acc: 0.8983\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1040 - acc: 0.9642 - val_loss: 0.3129 - val_acc: 0.8990\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1045 - acc: 0.9637 - val_loss: 0.3095 - val_acc: 0.9005\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3160 - acc: 0.8911\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 59us/sample - loss: 0.6142 - acc: 0.7926 - val_loss: 0.4958 - val_acc: 0.8223\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.4309 - acc: 0.8469 - val_loss: 0.4058 - val_acc: 0.8522\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3969 - acc: 0.8575 - val_loss: 0.3858 - val_acc: 0.8607\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.3712 - acc: 0.8659 - val_loss: 0.3732 - val_acc: 0.8637\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3492 - acc: 0.8738 - val_loss: 0.3646 - val_acc: 0.8678\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3356 - acc: 0.8791 - val_loss: 0.3373 - val_acc: 0.8763\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.3211 - acc: 0.8836 - val_loss: 0.3354 - val_acc: 0.8765\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 3s 49us/sample - loss: 0.3089 - acc: 0.8889 - val_loss: 0.3285 - val_acc: 0.8847\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.2973 - acc: 0.8917 - val_loss: 0.3355 - val_acc: 0.8785\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2887 - acc: 0.8946 - val_loss: 0.3345 - val_acc: 0.8803\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2784 - acc: 0.8980 - val_loss: 0.3108 - val_acc: 0.8873\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2686 - acc: 0.9021 - val_loss: 0.3196 - val_acc: 0.8862\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2606 - acc: 0.9046 - val_loss: 0.3126 - val_acc: 0.8883\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2546 - acc: 0.9055 - val_loss: 0.3042 - val_acc: 0.8907\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2435 - acc: 0.9112 - val_loss: 0.3075 - val_acc: 0.8858\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2377 - acc: 0.9130 - val_loss: 0.3046 - val_acc: 0.8888\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2306 - acc: 0.9154 - val_loss: 0.3070 - val_acc: 0.8913\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2232 - acc: 0.9179 - val_loss: 0.3080 - val_acc: 0.8910\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2195 - acc: 0.9190 - val_loss: 0.2937 - val_acc: 0.8938\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2118 - acc: 0.9235 - val_loss: 0.2978 - val_acc: 0.8893\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2066 - acc: 0.9251 - val_loss: 0.2956 - val_acc: 0.8918\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2006 - acc: 0.9271 - val_loss: 0.3016 - val_acc: 0.8897\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1946 - acc: 0.9288 - val_loss: 0.2981 - val_acc: 0.8932\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1901 - acc: 0.9306 - val_loss: 0.2904 - val_acc: 0.8978\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1845 - acc: 0.9334 - val_loss: 0.3105 - val_acc: 0.8910\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1802 - acc: 0.9344 - val_loss: 0.2911 - val_acc: 0.8935\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1738 - acc: 0.9377 - val_loss: 0.2942 - val_acc: 0.8965\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1685 - acc: 0.9391 - val_loss: 0.2891 - val_acc: 0.8990\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1680 - acc: 0.9393 - val_loss: 0.2940 - val_acc: 0.8970\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1622 - acc: 0.9417 - val_loss: 0.2980 - val_acc: 0.8952\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1575 - acc: 0.9431 - val_loss: 0.3006 - val_acc: 0.8937\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1522 - acc: 0.9453 - val_loss: 0.3001 - val_acc: 0.8955\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1496 - acc: 0.9462 - val_loss: 0.3060 - val_acc: 0.8933\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1458 - acc: 0.9470 - val_loss: 0.3158 - val_acc: 0.8878\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1416 - acc: 0.9495 - val_loss: 0.2994 - val_acc: 0.8977\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1381 - acc: 0.9512 - val_loss: 0.2958 - val_acc: 0.8988\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1329 - acc: 0.9524 - val_loss: 0.2978 - val_acc: 0.9010\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1314 - acc: 0.9534 - val_loss: 0.3050 - val_acc: 0.8955\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1267 - acc: 0.9546 - val_loss: 0.3070 - val_acc: 0.8962\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1237 - acc: 0.9563 - val_loss: 0.3046 - val_acc: 0.8962\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1179 - acc: 0.9586 - val_loss: 0.3095 - val_acc: 0.8980\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1155 - acc: 0.9596 - val_loss: 0.3028 - val_acc: 0.8995\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1126 - acc: 0.9611 - val_loss: 0.3335 - val_acc: 0.8927\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1108 - acc: 0.9617 - val_loss: 0.3189 - val_acc: 0.8932\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1061 - acc: 0.9626 - val_loss: 0.3127 - val_acc: 0.8952\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1058 - acc: 0.9630 - val_loss: 0.3189 - val_acc: 0.8943\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1019 - acc: 0.9640 - val_loss: 0.3195 - val_acc: 0.8955\n",
      "Epoch 48/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.0976 - acc: 0.9661 - val_loss: 0.3167 - val_acc: 0.8982\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.3110 - acc: 0.8922\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 63us/sample - loss: 0.6070 - acc: 0.7939 - val_loss: 0.4655 - val_acc: 0.8327\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.4308 - acc: 0.8462 - val_loss: 0.4048 - val_acc: 0.8542\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3939 - acc: 0.8603 - val_loss: 0.3823 - val_acc: 0.8652\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.3699 - acc: 0.8667 - val_loss: 0.3676 - val_acc: 0.8640\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.3491 - acc: 0.8741 - val_loss: 0.3556 - val_acc: 0.8720\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3337 - acc: 0.8783 - val_loss: 0.3444 - val_acc: 0.8760\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3192 - acc: 0.8850 - val_loss: 0.3551 - val_acc: 0.8710\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.3078 - acc: 0.8869 - val_loss: 0.3384 - val_acc: 0.8787\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2991 - acc: 0.8921 - val_loss: 0.3300 - val_acc: 0.8783\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.2859 - acc: 0.8964 - val_loss: 0.3135 - val_acc: 0.8852\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2759 - acc: 0.8993 - val_loss: 0.3186 - val_acc: 0.8857\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2691 - acc: 0.9005 - val_loss: 0.3078 - val_acc: 0.8875\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2593 - acc: 0.9046 - val_loss: 0.3058 - val_acc: 0.8903\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2524 - acc: 0.9065 - val_loss: 0.3058 - val_acc: 0.8902\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2454 - acc: 0.9091 - val_loss: 0.2927 - val_acc: 0.8943\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2359 - acc: 0.9144 - val_loss: 0.2938 - val_acc: 0.8942\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2308 - acc: 0.9150 - val_loss: 0.2921 - val_acc: 0.8948\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2247 - acc: 0.9177 - val_loss: 0.2930 - val_acc: 0.8953\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2172 - acc: 0.9203 - val_loss: 0.2947 - val_acc: 0.8905\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2116 - acc: 0.9226 - val_loss: 0.3182 - val_acc: 0.8850\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2053 - acc: 0.9257 - val_loss: 0.2985 - val_acc: 0.8897\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2015 - acc: 0.9267 - val_loss: 0.2894 - val_acc: 0.8942\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1949 - acc: 0.9289 - val_loss: 0.2986 - val_acc: 0.8935\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1893 - acc: 0.9311 - val_loss: 0.2956 - val_acc: 0.8938\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1851 - acc: 0.9326 - val_loss: 0.2905 - val_acc: 0.8978\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1791 - acc: 0.9355 - val_loss: 0.2930 - val_acc: 0.8967\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1745 - acc: 0.9370 - val_loss: 0.2884 - val_acc: 0.9003\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1705 - acc: 0.9386 - val_loss: 0.2990 - val_acc: 0.8928\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 3s 54us/sample - loss: 0.1660 - acc: 0.9404 - val_loss: 0.2959 - val_acc: 0.8957\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1605 - acc: 0.9426 - val_loss: 0.2972 - val_acc: 0.8953\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1580 - acc: 0.9438 - val_loss: 0.2928 - val_acc: 0.8977\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1535 - acc: 0.9452 - val_loss: 0.2981 - val_acc: 0.8968\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.1486 - acc: 0.9466 - val_loss: 0.2985 - val_acc: 0.8980\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1464 - acc: 0.9470 - val_loss: 0.3034 - val_acc: 0.8968\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.1441 - acc: 0.9482 - val_loss: 0.3202 - val_acc: 0.8880\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1379 - acc: 0.9511 - val_loss: 0.2956 - val_acc: 0.8958\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1341 - acc: 0.9522 - val_loss: 0.2960 - val_acc: 0.8982\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1324 - acc: 0.9526 - val_loss: 0.3030 - val_acc: 0.8937\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1264 - acc: 0.9551 - val_loss: 0.2967 - val_acc: 0.9018\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1236 - acc: 0.9568 - val_loss: 0.2971 - val_acc: 0.8988\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1225 - acc: 0.9582 - val_loss: 0.3006 - val_acc: 0.9013\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1175 - acc: 0.9583 - val_loss: 0.3023 - val_acc: 0.8985\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1131 - acc: 0.9604 - val_loss: 0.3082 - val_acc: 0.8990\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1103 - acc: 0.9615 - val_loss: 0.3123 - val_acc: 0.8958\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1078 - acc: 0.9621 - val_loss: 0.3239 - val_acc: 0.8938\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1042 - acc: 0.9638 - val_loss: 0.3127 - val_acc: 0.8990\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1011 - acc: 0.9654 - val_loss: 0.3151 - val_acc: 0.8997\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.3141 - acc: 0.8919\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 62us/sample - loss: 0.6106 - acc: 0.7923 - val_loss: 0.4716 - val_acc: 0.8310\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.4353 - acc: 0.8446 - val_loss: 0.4188 - val_acc: 0.8512\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3956 - acc: 0.8576 - val_loss: 0.3845 - val_acc: 0.8628\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.3689 - acc: 0.8671 - val_loss: 0.3773 - val_acc: 0.8623\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3517 - acc: 0.8735 - val_loss: 0.3542 - val_acc: 0.8758\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3349 - acc: 0.8794 - val_loss: 0.3543 - val_acc: 0.8707\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3216 - acc: 0.8827 - val_loss: 0.3378 - val_acc: 0.8777\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.3081 - acc: 0.8873 - val_loss: 0.3276 - val_acc: 0.8813\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2967 - acc: 0.8919 - val_loss: 0.3237 - val_acc: 0.8832\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2877 - acc: 0.8955 - val_loss: 0.3151 - val_acc: 0.8860\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2771 - acc: 0.8987 - val_loss: 0.3135 - val_acc: 0.8833\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2680 - acc: 0.9013 - val_loss: 0.3083 - val_acc: 0.8875\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2596 - acc: 0.9040 - val_loss: 0.3117 - val_acc: 0.8855\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2527 - acc: 0.9078 - val_loss: 0.3009 - val_acc: 0.8907\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2420 - acc: 0.9097 - val_loss: 0.3000 - val_acc: 0.8883\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2376 - acc: 0.9132 - val_loss: 0.3079 - val_acc: 0.8873\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.2302 - acc: 0.9152 - val_loss: 0.2925 - val_acc: 0.8952\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2234 - acc: 0.9175 - val_loss: 0.2962 - val_acc: 0.8917\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2185 - acc: 0.9189 - val_loss: 0.2978 - val_acc: 0.8893\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2110 - acc: 0.9223 - val_loss: 0.2957 - val_acc: 0.8897\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.2071 - acc: 0.9240 - val_loss: 0.2986 - val_acc: 0.8917\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1994 - acc: 0.9275 - val_loss: 0.2903 - val_acc: 0.8955\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1943 - acc: 0.9290 - val_loss: 0.2921 - val_acc: 0.8912\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1891 - acc: 0.9318 - val_loss: 0.2899 - val_acc: 0.8958\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1865 - acc: 0.9322 - val_loss: 0.2998 - val_acc: 0.8902\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1803 - acc: 0.9331 - val_loss: 0.2893 - val_acc: 0.8920\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1766 - acc: 0.9367 - val_loss: 0.2893 - val_acc: 0.8937\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1703 - acc: 0.9384 - val_loss: 0.2969 - val_acc: 0.8917\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1645 - acc: 0.9405 - val_loss: 0.2915 - val_acc: 0.8963\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1612 - acc: 0.9422 - val_loss: 0.2955 - val_acc: 0.8950\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1570 - acc: 0.9436 - val_loss: 0.2941 - val_acc: 0.8975\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1546 - acc: 0.9444 - val_loss: 0.3030 - val_acc: 0.8905\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1502 - acc: 0.9457 - val_loss: 0.3021 - val_acc: 0.8918\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1445 - acc: 0.9481 - val_loss: 0.3019 - val_acc: 0.8937\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1410 - acc: 0.9498 - val_loss: 0.2874 - val_acc: 0.8978\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1392 - acc: 0.9495 - val_loss: 0.3042 - val_acc: 0.8945\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1329 - acc: 0.9528 - val_loss: 0.2928 - val_acc: 0.8998\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1300 - acc: 0.9532 - val_loss: 0.2988 - val_acc: 0.8972\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1272 - acc: 0.9546 - val_loss: 0.3061 - val_acc: 0.8947\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1229 - acc: 0.9566 - val_loss: 0.2930 - val_acc: 0.9005\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1203 - acc: 0.9570 - val_loss: 0.3028 - val_acc: 0.8975\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1162 - acc: 0.9585 - val_loss: 0.3149 - val_acc: 0.8943\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1140 - acc: 0.9599 - val_loss: 0.3028 - val_acc: 0.8988\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1106 - acc: 0.9614 - val_loss: 0.3125 - val_acc: 0.8963\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1080 - acc: 0.9625 - val_loss: 0.3011 - val_acc: 0.9003\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1043 - acc: 0.9640 - val_loss: 0.3087 - val_acc: 0.8983\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.0996 - acc: 0.9658 - val_loss: 0.3188 - val_acc: 0.8937\n",
      "Epoch 48/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.1001 - acc: 0.9652 - val_loss: 0.3078 - val_acc: 0.8978\n",
      "Epoch 49/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.0957 - acc: 0.9669 - val_loss: 0.3124 - val_acc: 0.8980\n",
      "Epoch 50/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.0919 - acc: 0.9689 - val_loss: 0.3196 - val_acc: 0.8988\n",
      "Epoch 51/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.0901 - acc: 0.9687 - val_loss: 0.3148 - val_acc: 0.8970\n",
      "Epoch 52/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.0874 - acc: 0.9704 - val_loss: 0.3357 - val_acc: 0.8962\n",
      "Epoch 53/500\n",
      "54000/54000 [==============================] - 2s 33us/sample - loss: 0.0842 - acc: 0.9717 - val_loss: 0.3289 - val_acc: 0.8965\n",
      "Epoch 54/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.0809 - acc: 0.9729 - val_loss: 0.3207 - val_acc: 0.8990\n",
      "Epoch 55/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.0808 - acc: 0.9732 - val_loss: 0.3131 - val_acc: 0.9012\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 0.3166 - acc: 0.8934\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 4s 68us/sample - loss: 0.6219 - acc: 0.7886 - val_loss: 0.4696 - val_acc: 0.8338\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.4328 - acc: 0.8461 - val_loss: 0.4056 - val_acc: 0.8547\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.3944 - acc: 0.8600 - val_loss: 0.3860 - val_acc: 0.8593\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3694 - acc: 0.8679 - val_loss: 0.3685 - val_acc: 0.8677\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3512 - acc: 0.8739 - val_loss: 0.3504 - val_acc: 0.8715\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3349 - acc: 0.8796 - val_loss: 0.3426 - val_acc: 0.8792\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.3230 - acc: 0.8825 - val_loss: 0.3369 - val_acc: 0.8737\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3089 - acc: 0.8881 - val_loss: 0.3302 - val_acc: 0.8828\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2967 - acc: 0.8921 - val_loss: 0.3270 - val_acc: 0.8832\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2867 - acc: 0.8950 - val_loss: 0.3199 - val_acc: 0.8842\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2769 - acc: 0.8990 - val_loss: 0.3197 - val_acc: 0.8840\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2686 - acc: 0.9018 - val_loss: 0.3054 - val_acc: 0.8895\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2573 - acc: 0.9054 - val_loss: 0.3016 - val_acc: 0.8907\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2514 - acc: 0.9072 - val_loss: 0.3047 - val_acc: 0.8895\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2433 - acc: 0.9114 - val_loss: 0.2977 - val_acc: 0.8912\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2366 - acc: 0.9138 - val_loss: 0.3076 - val_acc: 0.8867\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.2295 - acc: 0.9161 - val_loss: 0.3033 - val_acc: 0.8903\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2233 - acc: 0.9181 - val_loss: 0.2994 - val_acc: 0.8893\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2160 - acc: 0.9223 - val_loss: 0.2950 - val_acc: 0.8957\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2108 - acc: 0.9228 - val_loss: 0.2868 - val_acc: 0.8943\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.2062 - acc: 0.9243 - val_loss: 0.2884 - val_acc: 0.8963\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1997 - acc: 0.9272 - val_loss: 0.2967 - val_acc: 0.8938\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1939 - acc: 0.9287 - val_loss: 0.2901 - val_acc: 0.8912\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1907 - acc: 0.9315 - val_loss: 0.2916 - val_acc: 0.8928\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1828 - acc: 0.9340 - val_loss: 0.2888 - val_acc: 0.8945\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1788 - acc: 0.9353 - val_loss: 0.2971 - val_acc: 0.8895\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1748 - acc: 0.9364 - val_loss: 0.2922 - val_acc: 0.8928\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1697 - acc: 0.9383 - val_loss: 0.2907 - val_acc: 0.8972\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.1654 - acc: 0.9407 - val_loss: 0.2897 - val_acc: 0.8948\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1605 - acc: 0.9420 - val_loss: 0.2873 - val_acc: 0.8980\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1559 - acc: 0.9439 - val_loss: 0.3018 - val_acc: 0.8903\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1525 - acc: 0.9449 - val_loss: 0.2879 - val_acc: 0.8972\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1474 - acc: 0.9478 - val_loss: 0.2961 - val_acc: 0.8955\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1438 - acc: 0.9490 - val_loss: 0.2964 - val_acc: 0.8952\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1390 - acc: 0.9504 - val_loss: 0.3006 - val_acc: 0.8957\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1370 - acc: 0.9512 - val_loss: 0.2984 - val_acc: 0.8980\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1349 - acc: 0.9517 - val_loss: 0.3036 - val_acc: 0.8962\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1296 - acc: 0.9543 - val_loss: 0.2913 - val_acc: 0.8960\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1248 - acc: 0.9562 - val_loss: 0.2990 - val_acc: 0.8955\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1247 - acc: 0.9556 - val_loss: 0.3188 - val_acc: 0.8953\n",
      "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3163 - acc: 0.8892\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 3s 63us/sample - loss: 0.6066 - acc: 0.7958 - val_loss: 0.4557 - val_acc: 0.8383\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.4327 - acc: 0.8460 - val_loss: 0.4075 - val_acc: 0.8507\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.3917 - acc: 0.8600 - val_loss: 0.3875 - val_acc: 0.8588\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.3700 - acc: 0.8671 - val_loss: 0.3638 - val_acc: 0.8698\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.3483 - acc: 0.8747 - val_loss: 0.3599 - val_acc: 0.8670\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.3345 - acc: 0.8793 - val_loss: 0.3437 - val_acc: 0.8767\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3188 - acc: 0.8842 - val_loss: 0.3645 - val_acc: 0.8642\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.3081 - acc: 0.8875 - val_loss: 0.3317 - val_acc: 0.8783\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.2970 - acc: 0.8909 - val_loss: 0.3201 - val_acc: 0.8855\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2848 - acc: 0.8956 - val_loss: 0.3279 - val_acc: 0.8790\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2767 - acc: 0.8993 - val_loss: 0.3234 - val_acc: 0.8803\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2657 - acc: 0.9021 - val_loss: 0.3046 - val_acc: 0.8905\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.2608 - acc: 0.9041 - val_loss: 0.3048 - val_acc: 0.8908\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.2513 - acc: 0.9074 - val_loss: 0.3021 - val_acc: 0.8902\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2443 - acc: 0.9094 - val_loss: 0.3024 - val_acc: 0.8908\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2361 - acc: 0.9126 - val_loss: 0.3071 - val_acc: 0.8905\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2316 - acc: 0.9147 - val_loss: 0.3004 - val_acc: 0.8877\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2249 - acc: 0.9170 - val_loss: 0.2919 - val_acc: 0.8918\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.2170 - acc: 0.9208 - val_loss: 0.2959 - val_acc: 0.8915\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2118 - acc: 0.9229 - val_loss: 0.2930 - val_acc: 0.8927\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.2054 - acc: 0.9247 - val_loss: 0.3016 - val_acc: 0.8895\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1996 - acc: 0.9267 - val_loss: 0.3028 - val_acc: 0.8902\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1963 - acc: 0.9289 - val_loss: 0.2968 - val_acc: 0.8902\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1899 - acc: 0.9306 - val_loss: 0.2970 - val_acc: 0.8928\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1858 - acc: 0.9327 - val_loss: 0.2954 - val_acc: 0.8925\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1799 - acc: 0.9343 - val_loss: 0.2931 - val_acc: 0.8932\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1760 - acc: 0.9361 - val_loss: 0.3009 - val_acc: 0.8898\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 2s 34us/sample - loss: 0.1696 - acc: 0.9388 - val_loss: 0.2988 - val_acc: 0.8927\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1654 - acc: 0.9407 - val_loss: 0.2833 - val_acc: 0.8973\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1611 - acc: 0.9419 - val_loss: 0.2899 - val_acc: 0.8958\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1573 - acc: 0.9431 - val_loss: 0.3014 - val_acc: 0.8948\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1543 - acc: 0.9444 - val_loss: 0.2921 - val_acc: 0.8962\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.1484 - acc: 0.9473 - val_loss: 0.2938 - val_acc: 0.8955\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1463 - acc: 0.9465 - val_loss: 0.2971 - val_acc: 0.8942\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1399 - acc: 0.9501 - val_loss: 0.2947 - val_acc: 0.8968\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1372 - acc: 0.9503 - val_loss: 0.2912 - val_acc: 0.8990\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1340 - acc: 0.9519 - val_loss: 0.2907 - val_acc: 0.8967\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1309 - acc: 0.9535 - val_loss: 0.2975 - val_acc: 0.8982\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1252 - acc: 0.9556 - val_loss: 0.3199 - val_acc: 0.8937\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1236 - acc: 0.9567 - val_loss: 0.2904 - val_acc: 0.8983\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 2s 35us/sample - loss: 0.1176 - acc: 0.9591 - val_loss: 0.3089 - val_acc: 0.8932\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.1163 - acc: 0.9593 - val_loss: 0.3047 - val_acc: 0.8948\n",
      "Epoch 43/500\n",
      "54000/54000 [==============================] - 2s 44us/sample - loss: 0.1133 - acc: 0.9599 - val_loss: 0.2990 - val_acc: 0.8962\n",
      "Epoch 44/500\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.1114 - acc: 0.9609 - val_loss: 0.3070 - val_acc: 0.8977\n",
      "Epoch 45/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.1076 - acc: 0.9622 - val_loss: 0.3054 - val_acc: 0.9013\n",
      "Epoch 46/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1038 - acc: 0.9644 - val_loss: 0.3106 - val_acc: 0.8957\n",
      "Epoch 47/500\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.1014 - acc: 0.9645 - val_loss: 0.3098 - val_acc: 0.8985\n",
      "Epoch 48/500\n",
      "54000/54000 [==============================] - 2s 37us/sample - loss: 0.0985 - acc: 0.9657 - val_loss: 0.3055 - val_acc: 0.8993\n",
      "Epoch 49/500\n",
      "54000/54000 [==============================] - 2s 36us/sample - loss: 0.0949 - acc: 0.9674 - val_loss: 0.3282 - val_acc: 0.8945\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.3124 - acc: 0.8895\n",
      "0.8908649981021881 0.3148118580421806\n"
     ]
    }
   ],
   "source": [
    "## try one training\n",
    "nodes = 350 ; hl = 1 ; AccuracyTesting = 0 ; LossTesting = 0; nrep = 20;\n",
    "for i in range(nrep):\n",
    "    net = MLP_Create(nhidden1 = nodes)\n",
    "    path =  create_path(hl,nodes)\n",
    "    history_350 = Fit_network(net, path)\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    AccuracyTesting += test_acc\n",
    "    LossTesting += test_loss \n",
    "\n",
    "FinalAcc =AccuracyTesting/nrep\n",
    "FinalLoss = LossTesting/nrep\n",
    "print(FinalAcc, FinalLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8908649981021881 0.3148118580421806\n"
     ]
    }
   ],
   "source": [
    "print(FinalAcc, FinalLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/500\n",
      "54000/54000 [==============================] - 8s 147us/sample - loss: 0.5663 - acc: 0.8019 - val_loss: 0.4571 - val_acc: 0.8347\n",
      "Epoch 2/500\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.4261 - acc: 0.8473 - val_loss: 0.3982 - val_acc: 0.8550\n",
      "Epoch 3/500\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.3911 - acc: 0.8605 - val_loss: 0.3874 - val_acc: 0.8545\n",
      "Epoch 4/500\n",
      "54000/54000 [==============================] - 4s 77us/sample - loss: 0.3657 - acc: 0.8691 - val_loss: 0.3595 - val_acc: 0.8702\n",
      "Epoch 5/500\n",
      "54000/54000 [==============================] - 4s 76us/sample - loss: 0.3459 - acc: 0.8733 - val_loss: 0.3477 - val_acc: 0.8740\n",
      "Epoch 6/500\n",
      "54000/54000 [==============================] - 4s 77us/sample - loss: 0.3301 - acc: 0.8794 - val_loss: 0.3721 - val_acc: 0.8625\n",
      "Epoch 7/500\n",
      "54000/54000 [==============================] - 4s 77us/sample - loss: 0.3162 - acc: 0.8844 - val_loss: 0.3405 - val_acc: 0.8808\n",
      "Epoch 8/500\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.3003 - acc: 0.8891 - val_loss: 0.3435 - val_acc: 0.8743\n",
      "Epoch 9/500\n",
      "54000/54000 [==============================] - 4s 77us/sample - loss: 0.2851 - acc: 0.8952 - val_loss: 0.3302 - val_acc: 0.8807\n",
      "Epoch 10/500\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.2767 - acc: 0.8969 - val_loss: 0.3197 - val_acc: 0.8845\n",
      "Epoch 11/500\n",
      "54000/54000 [==============================] - 4s 77us/sample - loss: 0.2670 - acc: 0.9002 - val_loss: 0.3102 - val_acc: 0.8833\n",
      "Epoch 12/500\n",
      "54000/54000 [==============================] - 4s 75us/sample - loss: 0.2545 - acc: 0.9051 - val_loss: 0.3199 - val_acc: 0.8837\n",
      "Epoch 13/500\n",
      "54000/54000 [==============================] - 4s 77us/sample - loss: 0.2456 - acc: 0.9089 - val_loss: 0.3067 - val_acc: 0.8878\n",
      "Epoch 14/500\n",
      "54000/54000 [==============================] - 4s 76us/sample - loss: 0.2408 - acc: 0.9109 - val_loss: 0.3070 - val_acc: 0.8893\n",
      "Epoch 15/500\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.2281 - acc: 0.9149 - val_loss: 0.2969 - val_acc: 0.8912\n",
      "Epoch 16/500\n",
      "54000/54000 [==============================] - 4s 79us/sample - loss: 0.2217 - acc: 0.9178 - val_loss: 0.3018 - val_acc: 0.8913\n",
      "Epoch 17/500\n",
      "54000/54000 [==============================] - 4s 80us/sample - loss: 0.2134 - acc: 0.9199 - val_loss: 0.3004 - val_acc: 0.8927\n",
      "Epoch 18/500\n",
      "54000/54000 [==============================] - 4s 81us/sample - loss: 0.2055 - acc: 0.9236 - val_loss: 0.3011 - val_acc: 0.8895\n",
      "Epoch 19/500\n",
      "54000/54000 [==============================] - 4s 82us/sample - loss: 0.1985 - acc: 0.9264 - val_loss: 0.2961 - val_acc: 0.8908\n",
      "Epoch 20/500\n",
      "54000/54000 [==============================] - 4s 83us/sample - loss: 0.1908 - acc: 0.9294 - val_loss: 0.2957 - val_acc: 0.8940\n",
      "Epoch 21/500\n",
      "54000/54000 [==============================] - 4s 83us/sample - loss: 0.1846 - acc: 0.9319 - val_loss: 0.2912 - val_acc: 0.8953\n",
      "Epoch 22/500\n",
      "54000/54000 [==============================] - 5s 89us/sample - loss: 0.1794 - acc: 0.9336 - val_loss: 0.2897 - val_acc: 0.8993\n",
      "Epoch 23/500\n",
      "54000/54000 [==============================] - 5s 89us/sample - loss: 0.1742 - acc: 0.9350 - val_loss: 0.3139 - val_acc: 0.8898\n",
      "Epoch 24/500\n",
      "54000/54000 [==============================] - 5s 87us/sample - loss: 0.1660 - acc: 0.9388 - val_loss: 0.2924 - val_acc: 0.8967\n",
      "Epoch 25/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.1620 - acc: 0.9395 - val_loss: 0.2944 - val_acc: 0.8955\n",
      "Epoch 26/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.1537 - acc: 0.9426 - val_loss: 0.2933 - val_acc: 0.8972\n",
      "Epoch 27/500\n",
      "54000/54000 [==============================] - 5s 92us/sample - loss: 0.1492 - acc: 0.9444 - val_loss: 0.3136 - val_acc: 0.8923\n",
      "Epoch 28/500\n",
      "54000/54000 [==============================] - 5s 90us/sample - loss: 0.1488 - acc: 0.9444 - val_loss: 0.3331 - val_acc: 0.8867\n",
      "Epoch 29/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.1405 - acc: 0.9486 - val_loss: 0.2942 - val_acc: 0.8975\n",
      "Epoch 30/500\n",
      "54000/54000 [==============================] - 5s 91us/sample - loss: 0.1339 - acc: 0.9514 - val_loss: 0.3304 - val_acc: 0.8882\n",
      "Epoch 31/500\n",
      "54000/54000 [==============================] - 5s 95us/sample - loss: 0.1294 - acc: 0.9532 - val_loss: 0.3139 - val_acc: 0.8938\n",
      "Epoch 32/500\n",
      "54000/54000 [==============================] - 5s 92us/sample - loss: 0.1269 - acc: 0.9534 - val_loss: 0.3115 - val_acc: 0.8985\n",
      "Epoch 33/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.1206 - acc: 0.9564 - val_loss: 0.2999 - val_acc: 0.8987\n",
      "Epoch 34/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.1169 - acc: 0.9576 - val_loss: 0.3047 - val_acc: 0.8988\n",
      "Epoch 35/500\n",
      "54000/54000 [==============================] - 5s 89us/sample - loss: 0.1131 - acc: 0.9592 - val_loss: 0.3185 - val_acc: 0.8962\n",
      "Epoch 36/500\n",
      "54000/54000 [==============================] - 5s 89us/sample - loss: 0.1059 - acc: 0.9622 - val_loss: 0.3165 - val_acc: 0.8982\n",
      "Epoch 37/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.1045 - acc: 0.9616 - val_loss: 0.3281 - val_acc: 0.8958\n",
      "Epoch 38/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.0999 - acc: 0.9643 - val_loss: 0.3369 - val_acc: 0.8965\n",
      "Epoch 39/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.1009 - acc: 0.9632 - val_loss: 0.3439 - val_acc: 0.8960\n",
      "Epoch 40/500\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.0932 - acc: 0.9668 - val_loss: 0.3509 - val_acc: 0.8915\n",
      "Epoch 41/500\n",
      "54000/54000 [==============================] - 5s 89us/sample - loss: 0.0885 - acc: 0.9681 - val_loss: 0.3370 - val_acc: 0.8987\n",
      "Epoch 42/500\n",
      "54000/54000 [==============================] - 5s 87us/sample - loss: 0.0856 - acc: 0.9703 - val_loss: 0.3306 - val_acc: 0.8987\n"
     ]
    }
   ],
   "source": [
    "his = Fit_network(net, path)\n",
    "trained_network = load_model(path)\n",
    " test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on learning rate  = 0.0001\n",
      "WARNING:tensorflow:From C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\skypi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.3123 - acc: 0.8897\n",
      "working on learning rate  = 0.001\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.3144 - acc: 0.8879\n",
      "working on learning rate  = 0.01\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.3825 - acc: 0.8686\n",
      "working on learning rate  = 0.1\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.6094 - acc: 0.8044\n",
      "working on learning rate  = 1\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 8.5289 - acc: 0.5759\n",
      "working on learning rate  = 10\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 179.7237 - acc: 0.3663\n"
     ]
    }
   ],
   "source": [
    "## Optimize for learning rate : \n",
    "# Looping over learning rates and storing the results \n",
    "LR_array = [1e-4, 1e-3, 1e-2, .1, 1, 10]\n",
    "# LR_array = [.1]\n",
    "batchnum = 128; StopCriteria = 'val_loss'; epochs = 500; nhidden = 350; nhiddenlayers = 1\n",
    "\n",
    "npoints = len(LR_array)\n",
    "d_lr = defaultdict(lambda: \"Not Present\") \n",
    "d_lr[\"train_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_lr[\"train_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_lr[\"val_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_lr[\"val_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "# make accuracy and loss list \n",
    "test_acc_diff_lr = np.zeros(npoints)\n",
    "test_loss_diff_lr = np.zeros(npoints)\n",
    "# loop over hidden layers \n",
    "for i in range(npoints):\n",
    "    LearningRate = LR_array[i]\n",
    "    path = create_path(nhiddenlayers,nhidden, lr = LearningRate)\n",
    "    print('working on learning rate  = '+ str(    LearningRate ))\n",
    "    #create network\n",
    "    network = MLP_Create(nhidden)\n",
    "    #fit\n",
    "    History = Fit_network(network, path , lr =LearningRate, verbose = 0)\n",
    "    \n",
    "    key = str(LearningRate)\n",
    "    d_lr[\"train_acc_arr\"][key] = History.history['acc']\n",
    "    d_lr[\"train_loss_arr\"][key]  = History.history['loss']\n",
    "    d_lr[\"val_acc_arr\"][key] = History.history['val_acc']\n",
    "    d_lr[\"val_loss_arr\"][key] = History.history['val_loss']\n",
    "    # test the network on the testing data\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    test_acc_diff_lr[i] = test_acc\n",
    "    test_loss_diff_lr[i] = test_loss\n",
    "\n",
    "#Get epochs array\n",
    "epoch_arr = np.zeros(npoints)\n",
    "i=0\n",
    "for key in d_lr[\"train_acc_arr\"]:\n",
    "    arr = d_lr[\"train_acc_arr\"][key]\n",
    "    epoch_arr[i] = len(arr) - 20\n",
    "    i=i+1\n",
    "\n",
    "    #######\n",
    "d_LR_TBS =defaultdict(lambda: \"Not Present\") \n",
    "d_LR_TBS[\"train_acc_arr\"] = dict(d_lr[\"train_acc_arr\"] )\n",
    "d_LR_TBS[\"train_loss_arr\"] = dict(d_lr[\"train_loss_arr\"])\n",
    "d_LR_TBS[\"val_acc_arr\"] = dict(d_lr[\"val_acc_arr\"])\n",
    "d_LR_TBS[\"val_loss_arr\"] = dict(d_lr[\"val_loss_arr\"])\n",
    "d_LR_TBS['test_acc'] = test_acc_diff_lr\n",
    "d_LR_TBS['test_loss'] = test_loss_diff_lr\n",
    "d_LR_TBS['epochs'] = epoch_arr \n",
    "d_LR_TBS['LR_vals'] = LR_array\n",
    "# d_LR_TBS['Description'] = 'Optimization for learning rate_Adam_1layer_ '+ str (nhidden) + '_nodes'\n",
    "d_LR_TBS = dict(d_LR_TBS)\n",
    "np.save('dict_LR_Adam_1layer_350_nodes', np.array(d_LR_TBS), allow_pickle = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2,figsize=(15,5), constrained_layout = True)\n",
    "epoch_arr = d_LR_TBS['epochs']\n",
    "test_arr = d_LR_TBS['test_acc'] \n",
    "\n",
    "a = np.arange(npoints)\n",
    "\n",
    "\n",
    "axs[0].bar(a, test_arr)\n",
    "axs[0].set_title('Testing Accuracy')\n",
    "# axs[0].scatter(a, test_arr)\n",
    "axs[0].set(xlabel='Learning Rate', ylabel = 'acc')\n",
    "axs[0].xaxis.set_ticks(a) #set the ticks to be a\n",
    "axs[0].xaxis.set_ticklabels(LR_array,rotation=45)\n",
    "# ####\n",
    "# plt.xticks(rotation='vertical')\n",
    "axs[1].bar(a, epoch_arr)\n",
    "axs[1].set_title('Epochs to train')\n",
    "axs[1].set(xlabel='Learning Rate', ylabel = 'epochs')\n",
    "axs[1].xaxis.set_ticks(a) #set the ticks to be a\n",
    "axs[1].xaxis.set_ticklabels(LR_array,rotation=45) # change the ticks' names to x\n",
    "\n",
    "fig.suptitle('Optimization for the learning rate on Adam optimizer ', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8897    , 0.88789999, 0.86860001, 0.80440003, 0.57590002,\n",
       "       0.36629999])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_diff_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization for Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Optimizer  = adam\n",
      "10000/10000 [==============================] - 2s 234us/sample - loss: 0.3126 - acc: 0.8915\n",
      "working on Optimizer  = SGD\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 0.4906 - acc: 0.8244\n",
      "working on Optimizer  = RMSProp\n",
      "10000/10000 [==============================] - 3s 258us/sample - loss: 0.3199 - acc: 0.8904\n",
      "working on Optimizer  = Adadelta\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.5565 - acc: 0.8062s -\n",
      "working on Optimizer  = Adagrad\n",
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.4676 - acc: 0.8345\n",
      "working on Optimizer  = Adamax\n",
      "10000/10000 [==============================] - 2s 210us/sample - loss: 0.3147 - acc: 0.8907\n"
     ]
    }
   ],
   "source": [
    "## Optimize for learning rate : \n",
    "# Looping over learning rates and storing the results \n",
    "Opt_Arr = ['adam', 'SGD', 'RMSProp', 'Adadelta', 'Adagrad','Adamax']\n",
    "# LR_array = [.1]\n",
    "batchnum = 128; StopCriteria = 'val_loss'; epochs = 500; nhidden = 350; nhiddenlayers = 1\n",
    "\n",
    "npoints = len(Opt_Arr)\n",
    "d_Opt = defaultdict(lambda: \"Not Present\") \n",
    "d_Opt[\"train_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_Opt[\"train_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_Opt[\"val_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_Opt[\"val_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "# make accuracy and loss list \n",
    "test_acc_Opt = np.zeros(npoints)\n",
    "test_loss_Opt = np.zeros(npoints)\n",
    "# loop over hidden layers \n",
    "for i in range(npoints):\n",
    "    opt = Opt_Arr[i]\n",
    "    key = opt\n",
    "    path = create_path(nhiddenlayers,nhidden,optimizer = opt)\n",
    "    print('working on Optimizer  = '+ opt)\n",
    "    #create network\n",
    "    network = MLP_Create(nhidden)\n",
    "    #fit\n",
    "    History = Fit_network(network, path , optimization = opt, verbose = 0)\n",
    "    \n",
    "#     key = str(LearningRate)\n",
    "    d_Opt[\"train_acc_arr\"][key] = History.history['acc']\n",
    "    d_Opt[\"train_loss_arr\"][key]  = History.history['loss']\n",
    "    d_Opt[\"val_acc_arr\"][key] = History.history['val_acc']\n",
    "    d_Opt[\"val_loss_arr\"][key] = History.history['val_loss']\n",
    "    # test the network on the testing data\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    test_acc_Opt[i] = test_acc\n",
    "    test_loss_Opt[i] = test_loss\n",
    "\n",
    "#Get epochs array\n",
    "epoch_arr = np.zeros(npoints)\n",
    "i=0\n",
    "for key in d_Opt[\"train_acc_arr\"]:\n",
    "    arr = d_Opt[\"train_acc_arr\"][key]\n",
    "    epoch_arr[i] = len(arr) - 20\n",
    "    i=i+1\n",
    "\n",
    "    #######\n",
    "d_Opt_TBS =defaultdict(lambda: \"Not Present\") \n",
    "d_Opt_TBS[\"train_acc_arr\"] = dict(d_Opt[\"train_acc_arr\"] )\n",
    "d_Opt_TBS[\"train_loss_arr\"] = dict(d_Opt[\"train_loss_arr\"])\n",
    "d_Opt_TBS[\"val_acc_arr\"] = dict(d_Opt[\"val_acc_arr\"])\n",
    "d_Opt_TBS[\"val_loss_arr\"] = dict(d_Opt[\"val_loss_arr\"])\n",
    "d_Opt_TBS['test_acc'] = test_acc_Opt\n",
    "d_Opt_TBS['test_loss'] = test_loss_Opt\n",
    "d_Opt_TBS['epochs'] = epoch_arr \n",
    "d_Opt_TBS['opt_vals'] = Opt_Arr\n",
    "# d_LR_TBS['Description'] = 'Optimization for learning rate_Adam_1layer_ '+ str (nhidden) + '_nodes'\n",
    "d_Opt_TBS = dict(d_Opt_TBS)\n",
    "np.save('dict_Opt_1layer_350_nodes', np.array(d_Opt_TBS), allow_pickle = True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization for number of hidden layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with  = 1 hidden layers and nodes = 20\n",
      "10000/10000 [==============================] - 1s 77us/sample - loss: 0.3751 - acc: 0.8675\n",
      "working with  = 2 hidden layers and nodes = 20\n",
      "10000/10000 [==============================] - 1s 81us/sample - loss: 0.3725 - acc: 0.8675\n",
      "working with  = 3 hidden layers and nodes = 20\n",
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.3796 - acc: 0.8656\n",
      "working with  = 4 hidden layers and nodes = 20\n",
      "10000/10000 [==============================] - 1s 86us/sample - loss: 0.4031 - acc: 0.8619\n",
      "working with  = 5 hidden layers and nodes = 20\n",
      "10000/10000 [==============================] - 1s 91us/sample - loss: 0.4847 - acc: 0.8461\n",
      "working with  = 6 hidden layers and nodes = 20\n",
      "10000/10000 [==============================] - 1s 94us/sample - loss: 0.5402 - acc: 0.8312\n"
     ]
    }
   ],
   "source": [
    "## Optimize for learning rate : \n",
    "# Looping over learning rates and storing the results \n",
    "Layers_arr = np.array([1,2,3,4,5,6])\n",
    "# LR_array = [.1]\n",
    "batchnum = 128; StopCriteria = 'val_loss'; epochs = 500; nhidden = 20;\n",
    "\n",
    "npoints = len(Layers_arr)\n",
    "d_layers = defaultdict(lambda: \"Not Present\") \n",
    "d_layers[\"train_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"train_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "# make accuracy and loss list \n",
    "test_acc_layers = np.zeros(npoints)\n",
    "test_loss_d_layers = np.zeros(npoints)\n",
    "# loop over hidden layers \n",
    "for i in range(npoints):\n",
    "    Layers = Layers_arr[i]\n",
    "    arg_layer = Layers - 1\n",
    "    key = str(Layers)\n",
    "    path = create_path(nlayers = Layers, nnodes = nhidden )\n",
    "    print('working with  = '+ str(    Layers ) + ' hidden layers and nodes = ' + str(nhidden))\n",
    "    #create network\n",
    "#     network = MLP_Create(nhidden)\n",
    "    network  = MLP_Create(nhidden1 = nhidden,  nhidden_in = nhidden,  nHiddenMoreLayers = arg_layer)\n",
    "    #fit\n",
    "    History = Fit_network(network, path , verbose = 0)\n",
    "    d_layers[\"train_acc_arr\"][key] = History.history['acc']\n",
    "    d_layers[\"train_loss_arr\"][key]  = History.history['loss']\n",
    "    d_layers[\"val_acc_arr\"][key] = History.history['val_acc']\n",
    "    d_layers[\"val_loss_arr\"][key] = History.history['val_loss']\n",
    "    # test the network on the testing data\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    test_acc_layers[i] = test_acc\n",
    "    test_loss_d_layers[i] = test_loss\n",
    "\n",
    "#Get epochs array\n",
    "epoch_arr = np.zeros(npoints)\n",
    "i=0\n",
    "for key in d_layers[\"train_acc_arr\"]:\n",
    "    arr = d_layers[\"train_acc_arr\"][key]\n",
    "    epoch_arr[i] = len(arr) - 20\n",
    "    i=i+1\n",
    "\n",
    "    #######\n",
    "\n",
    "d_layers_TBS =defaultdict(lambda: \"Not Present\") \n",
    "d_layers_TBS[\"train_acc_arr\"] = dict(d_layers[\"train_acc_arr\"] )\n",
    "d_layers_TBS[\"train_loss_arr\"] = dict(d_layers[\"train_loss_arr\"])\n",
    "d_layers_TBS[\"val_acc_arr\"] = dict(d_layers[\"val_acc_arr\"])\n",
    "d_layers_TBS[\"val_loss_arr\"] = dict(d_layers[\"val_loss_arr\"])\n",
    "d_layers_TBS['test_acc'] = test_acc_layers\n",
    "d_layers_TBS['test_loss'] = test_loss_d_layers\n",
    "d_layers_TBS['epochs'] = epoch_arr \n",
    "d_layers_TBS['hidden_layers_vals'] = Layers_arr\n",
    "# d_LR_TBS['Description'] = 'Optimization for learning rate_Adam_1layer_ '+ str (nhidden) + '_nodes'\n",
    "d_layers_TBS = dict(d_layers_TBS)\n",
    "np.save('dict_Diff_hiddenLayers_20_nodes_Adam_0.001', np.array(d_layers_TBS), allow_pickle = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with  = 1 hidden layers and nodes = 40\n",
      "10000/10000 [==============================] - 1s 123us/sample - loss: 0.3421 - acc: 0.8808\n",
      "working with  = 2 hidden layers and nodes = 40\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.3462 - acc: 0.8785\n",
      "working with  = 3 hidden layers and nodes = 40\n",
      "10000/10000 [==============================] - 1s 133us/sample - loss: 0.3539 - acc: 0.8762\n",
      "working with  = 4 hidden layers and nodes = 40\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 0.3932 - acc: 0.8693s - loss: 0.4058 - ac\n",
      "working with  = 5 hidden layers and nodes = 40\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.4291 - acc: 0.8665\n",
      "working with  = 6 hidden layers and nodes = 40\n",
      "10000/10000 [==============================] - 2s 187us/sample - loss: 0.5239 - acc: 0.8357\n"
     ]
    }
   ],
   "source": [
    "## Optimize for learning rate : \n",
    "# Looping over learning rates and storing the results \n",
    "Layers_arr = np.array([1,2,3,4,5,6])\n",
    "# LR_array = [.1]\n",
    "batchnum = 128; StopCriteria = 'val_loss'; epochs = 500; nhidden = 40;\n",
    "\n",
    "npoints = len(Layers_arr)\n",
    "d_layers = defaultdict(lambda: \"Not Present\") \n",
    "d_layers[\"train_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"train_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "# make accuracy and loss list \n",
    "test_acc_layers = np.zeros(npoints)\n",
    "test_loss_d_layers = np.zeros(npoints)\n",
    "# loop over hidden layers \n",
    "for i in range(npoints):\n",
    "    Layers = Layers_arr[i]\n",
    "    arg_layer = Layers - 1\n",
    "    key = str(Layers)\n",
    "    path = create_path(nlayers = Layers, nnodes = nhidden )\n",
    "    print('working with  = '+ str(    Layers ) + ' hidden layers and nodes = ' + str(nhidden))\n",
    "    #create network\n",
    "#     network = MLP_Create(nhidden)\n",
    "    network  = MLP_Create(nhidden1 = nhidden,  nhidden_in = nhidden,  nHiddenMoreLayers = arg_layer)\n",
    "    #fit\n",
    "    History = Fit_network(network, path , verbose = 0)\n",
    "    d_layers[\"train_acc_arr\"][key] = History.history['acc']\n",
    "    d_layers[\"train_loss_arr\"][key]  = History.history['loss']\n",
    "    d_layers[\"val_acc_arr\"][key] = History.history['val_acc']\n",
    "    d_layers[\"val_loss_arr\"][key] = History.history['val_loss']\n",
    "    # test the network on the testing data\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    test_acc_layers[i] = test_acc\n",
    "    test_loss_d_layers[i] = test_loss\n",
    "\n",
    "#Get epochs array\n",
    "epoch_arr = np.zeros(npoints)\n",
    "i=0\n",
    "for key in d_layers[\"train_acc_arr\"]:\n",
    "    arr = d_layers[\"train_acc_arr\"][key]\n",
    "    epoch_arr[i] = len(arr) - 20\n",
    "    i=i+1\n",
    "\n",
    "    #######\n",
    "\n",
    "d_layers_TBS =defaultdict(lambda: \"Not Present\") \n",
    "d_layers_TBS[\"train_acc_arr\"] = dict(d_layers[\"train_acc_arr\"] )\n",
    "d_layers_TBS[\"train_loss_arr\"] = dict(d_layers[\"train_loss_arr\"])\n",
    "d_layers_TBS[\"val_acc_arr\"] = dict(d_layers[\"val_acc_arr\"])\n",
    "d_layers_TBS[\"val_loss_arr\"] = dict(d_layers[\"val_loss_arr\"])\n",
    "d_layers_TBS['test_acc'] = test_acc_layers\n",
    "d_layers_TBS['test_loss'] = test_loss_d_layers\n",
    "d_layers_TBS['epochs'] = epoch_arr \n",
    "d_layers_TBS['hidden_layers_vals'] = Layers_arr\n",
    "# d_LR_TBS['Description'] = 'Optimization for learning rate_Adam_1layer_ '+ str (nhidden) + '_nodes'\n",
    "d_layers_TBS = dict(d_layers_TBS)\n",
    "np.save('dict_Diff_hiddenLayers_40_nodes_Adam_0.001', np.array(d_layers_TBS), allow_pickle = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with  = 1 hidden layers and nodes = 60\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.3366 - acc: 0.8797\n",
      "working with  = 2 hidden layers and nodes = 60\n",
      "10000/10000 [==============================] - 1s 134us/sample - loss: 0.3347 - acc: 0.8812\n",
      "working with  = 3 hidden layers and nodes = 60\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 0.3441 - acc: 0.8784\n",
      "working with  = 4 hidden layers and nodes = 60\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.3628 - acc: 0.8730\n",
      "working with  = 5 hidden layers and nodes = 60\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.3917 - acc: 0.8715\n",
      "working with  = 6 hidden layers and nodes = 60\n",
      "10000/10000 [==============================] - 2s 193us/sample - loss: 0.5110 - acc: 0.8362\n"
     ]
    }
   ],
   "source": [
    "## Optimize for learning rate : \n",
    "# Looping over learning rates and storing the results \n",
    "Layers_arr = np.array([1,2,3,4,5,6])\n",
    "# LR_array = [.1]\n",
    "batchnum = 128; StopCriteria = 'val_loss'; epochs = 500; nhidden = 60;\n",
    "\n",
    "npoints = len(Layers_arr)\n",
    "d_layers = defaultdict(lambda: \"Not Present\") \n",
    "d_layers[\"train_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"train_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "# make accuracy and loss list \n",
    "test_acc_layers = np.zeros(npoints)\n",
    "test_loss_d_layers = np.zeros(npoints)\n",
    "# loop over hidden layers \n",
    "for i in range(npoints):\n",
    "    Layers = Layers_arr[i]\n",
    "    arg_layer = Layers - 1\n",
    "    key = str(Layers)\n",
    "    path = create_path(nlayers = Layers, nnodes = nhidden )\n",
    "    print('working with  = '+ str(    Layers ) + ' hidden layers and nodes = ' + str(nhidden))\n",
    "    #create network\n",
    "#     network = MLP_Create(nhidden)\n",
    "    network  = MLP_Create(nhidden1 = nhidden,  nhidden_in = nhidden,  nHiddenMoreLayers = arg_layer)\n",
    "    #fit\n",
    "    History = Fit_network(network, path , verbose = 0)\n",
    "    d_layers[\"train_acc_arr\"][key] = History.history['acc']\n",
    "    d_layers[\"train_loss_arr\"][key]  = History.history['loss']\n",
    "    d_layers[\"val_acc_arr\"][key] = History.history['val_acc']\n",
    "    d_layers[\"val_loss_arr\"][key] = History.history['val_loss']\n",
    "    # test the network on the testing data\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    test_acc_layers[i] = test_acc\n",
    "    test_loss_d_layers[i] = test_loss\n",
    "\n",
    "#Get epochs array\n",
    "epoch_arr = np.zeros(npoints)\n",
    "i=0\n",
    "for key in d_layers[\"train_acc_arr\"]:\n",
    "    arr = d_layers[\"train_acc_arr\"][key]\n",
    "    epoch_arr[i] = len(arr) - 20\n",
    "    i=i+1\n",
    "\n",
    "    #######\n",
    "\n",
    "d_layers_TBS =defaultdict(lambda: \"Not Present\") \n",
    "d_layers_TBS[\"train_acc_arr\"] = dict(d_layers[\"train_acc_arr\"] )\n",
    "d_layers_TBS[\"train_loss_arr\"] = dict(d_layers[\"train_loss_arr\"])\n",
    "d_layers_TBS[\"val_acc_arr\"] = dict(d_layers[\"val_acc_arr\"])\n",
    "d_layers_TBS[\"val_loss_arr\"] = dict(d_layers[\"val_loss_arr\"])\n",
    "d_layers_TBS['test_acc'] = test_acc_layers\n",
    "d_layers_TBS['test_loss'] = test_loss_d_layers\n",
    "d_layers_TBS['epochs'] = epoch_arr \n",
    "d_layers_TBS['hidden_layers_vals'] = Layers_arr\n",
    "# d_LR_TBS['Description'] = 'Optimization for learning rate_Adam_1layer_ '+ str (nhidden) + '_nodes'\n",
    "d_layers_TBS = dict(d_layers_TBS)\n",
    "np.save('dict_Diff_hiddenLayers_60_nodes_Adam_0.001', np.array(d_layers_TBS), allow_pickle = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with  = 1 hidden layers and nodes = 80\n",
      "10000/10000 [==============================] - 2s 214us/sample - loss: 0.3279 - acc: 0.8825\n",
      "working with  = 2 hidden layers and nodes = 80\n",
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.3354 - acc: 0.8832\n",
      "working with  = 3 hidden layers and nodes = 80\n",
      "10000/10000 [==============================] - 2s 174us/sample - loss: 0.3328 - acc: 0.8817\n",
      "working with  = 4 hidden layers and nodes = 80\n",
      "10000/10000 [==============================] - 2s 197us/sample - loss: 0.3546 - acc: 0.8791\n",
      "working with  = 5 hidden layers and nodes = 80\n",
      "10000/10000 [==============================] - 2s 198us/sample - loss: 0.3756 - acc: 0.8748\n",
      "working with  = 6 hidden layers and nodes = 80\n",
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.4139 - acc: 0.8604\n"
     ]
    }
   ],
   "source": [
    "## Optimize for learning rate : \n",
    "# Looping over learning rates and storing the results \n",
    "Layers_arr = np.array([1,2,3,4,5,6])\n",
    "# LR_array = [.1]\n",
    "batchnum = 128; StopCriteria = 'val_loss'; epochs = 500; nhidden = 80;\n",
    "\n",
    "npoints = len(Layers_arr)\n",
    "d_layers = defaultdict(lambda: \"Not Present\") \n",
    "d_layers[\"train_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"train_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "# make accuracy and loss list \n",
    "test_acc_layers = np.zeros(npoints)\n",
    "test_loss_d_layers = np.zeros(npoints)\n",
    "# loop over hidden layers \n",
    "for i in range(npoints):\n",
    "    Layers = Layers_arr[i]\n",
    "    arg_layer = Layers - 1\n",
    "    key = str(Layers)\n",
    "    path = create_path(nlayers = Layers, nnodes = nhidden )\n",
    "    print('working with  = '+ str(    Layers ) + ' hidden layers and nodes = ' + str(nhidden))\n",
    "    #create network\n",
    "#     network = MLP_Create(nhidden)\n",
    "    network  = MLP_Create(nhidden1 = nhidden,  nhidden_in = nhidden,  nHiddenMoreLayers = arg_layer)\n",
    "    #fit\n",
    "    History = Fit_network(network, path , verbose = 0)\n",
    "    d_layers[\"train_acc_arr\"][key] = History.history['acc']\n",
    "    d_layers[\"train_loss_arr\"][key]  = History.history['loss']\n",
    "    d_layers[\"val_acc_arr\"][key] = History.history['val_acc']\n",
    "    d_layers[\"val_loss_arr\"][key] = History.history['val_loss']\n",
    "    # test the network on the testing data\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    test_acc_layers[i] = test_acc\n",
    "    test_loss_d_layers[i] = test_loss\n",
    "\n",
    "#Get epochs array\n",
    "epoch_arr = np.zeros(npoints)\n",
    "i=0\n",
    "for key in d_layers[\"train_acc_arr\"]:\n",
    "    arr = d_layers[\"train_acc_arr\"][key]\n",
    "    epoch_arr[i] = len(arr) - 20\n",
    "    i=i+1\n",
    "\n",
    "    #######\n",
    "\n",
    "d_layers_TBS =defaultdict(lambda: \"Not Present\") \n",
    "d_layers_TBS[\"train_acc_arr\"] = dict(d_layers[\"train_acc_arr\"] )\n",
    "d_layers_TBS[\"train_loss_arr\"] = dict(d_layers[\"train_loss_arr\"])\n",
    "d_layers_TBS[\"val_acc_arr\"] = dict(d_layers[\"val_acc_arr\"])\n",
    "d_layers_TBS[\"val_loss_arr\"] = dict(d_layers[\"val_loss_arr\"])\n",
    "d_layers_TBS['test_acc'] = test_acc_layers\n",
    "d_layers_TBS['test_loss'] = test_loss_d_layers\n",
    "d_layers_TBS['epochs'] = epoch_arr \n",
    "d_layers_TBS['hidden_layers_vals'] = Layers_arr\n",
    "# d_LR_TBS['Description'] = 'Optimization for learning rate_Adam_1layer_ '+ str (nhidden) + '_nodes'\n",
    "d_layers_TBS = dict(d_layers_TBS)\n",
    "np.save('dict_Diff_hiddenLayers_80_nodes_Adam_0.001', np.array(d_layers_TBS), allow_pickle = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize for learning rate : \n",
    "# Looping over learning rates and storing the results \n",
    "Layers_arr = np.array([1,2,3,4,5,6])\n",
    "# LR_array = [.1]\n",
    "batchnum = 128; StopCriteria = 'val_loss'; epochs = 500; nhidden = 350;\n",
    "\n",
    "npoints = len(Layers_arr)\n",
    "d_layers = defaultdict(lambda: \"Not Present\") \n",
    "d_layers[\"train_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"train_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_acc_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "d_layers[\"val_loss_arr\"] = defaultdict(lambda: \"Not Present\")\n",
    "# make accuracy and loss list \n",
    "test_acc_layers = np.zeros(npoints)\n",
    "test_loss_d_layers = np.zeros(npoints)\n",
    "# loop over hidden layers \n",
    "for i in range(npoints):\n",
    "    Layers = Layers_arr[i]\n",
    "    arg_layer = Layers - 1\n",
    "    key = str(Layers)\n",
    "    path = create_path(nlayers = Layers, nnodes = nhidden )\n",
    "    print('working with  = '+ str(    Layers ) + ' hidden layers and nodes = ' + str(nhidden))\n",
    "    #create network\n",
    "#     network = MLP_Create(nhidden)\n",
    "    network  = MLP_Create(nhidden1 = nhidden,  nhidden_in = nhidden,  nHiddenMoreLayers = arg_layer)\n",
    "    #fit\n",
    "    History = Fit_network(network, path , verbose = 0)\n",
    "    d_layers[\"train_acc_arr\"][key] = History.history['acc']\n",
    "    d_layers[\"train_loss_arr\"][key]  = History.history['loss']\n",
    "    d_layers[\"val_acc_arr\"][key] = History.history['val_acc']\n",
    "    d_layers[\"val_loss_arr\"][key] = History.history['val_loss']\n",
    "    # test the network on the testing data\n",
    "    trained_network = load_model(path)\n",
    "    test_loss, test_acc = trained_network.evaluate(XTest,YTest_oneHot)\n",
    "    test_acc_layers[i] = test_acc\n",
    "    test_loss_d_layers[i] = test_loss\n",
    "\n",
    "#Get epochs array\n",
    "epoch_arr = np.zeros(npoints)\n",
    "i=0\n",
    "for key in d_layers[\"train_acc_arr\"]:\n",
    "    arr = d_layers[\"train_acc_arr\"][key]\n",
    "    epoch_arr[i] = len(arr) - 20\n",
    "    i=i+1\n",
    "\n",
    "    #######\n",
    "\n",
    "d_layers_TBS =defaultdict(lambda: \"Not Present\") \n",
    "d_layers_TBS[\"train_acc_arr\"] = dict(d_layers[\"train_acc_arr\"] )\n",
    "d_layers_TBS[\"train_loss_arr\"] = dict(d_layers[\"train_loss_arr\"])\n",
    "d_layers_TBS[\"val_acc_arr\"] = dict(d_layers[\"val_acc_arr\"])\n",
    "d_layers_TBS[\"val_loss_arr\"] = dict(d_layers[\"val_loss_arr\"])\n",
    "d_layers_TBS['test_acc'] = test_acc_layers\n",
    "d_layers_TBS['test_loss'] = test_loss_d_layers\n",
    "d_layers_TBS['epochs'] = epoch_arr \n",
    "d_layers_TBS['hidden_layers_vals'] = Layers_arr\n",
    "# d_LR_TBS['Description'] = 'Optimization for learning rate_Adam_1layer_ '+ str (nhidden) + '_nodes'\n",
    "d_layers_TBS = dict(d_layers_TBS)\n",
    "np.save('dict_Diff_hiddenLayers_350_nodes_Adam_0.001', np.array(d_layers_TBS), allow_pickle = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
